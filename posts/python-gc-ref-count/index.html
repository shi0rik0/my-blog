<!DOCTYPE html><html> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><!-- KaTeX --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css" integrity="sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib" crossorigin="anonymous"><!-- Prism --><link href="/lib/prism.css" rel="stylesheet"><script src="/lib/prism.js"></script><meta name="generator" content="Astro v5.1.7"><title></title><link rel="stylesheet" href="/_astro/index.CsOifZER.css"></head> <body class="bg-[#333] text-[#ddd]"> <h1>Sustie</h1> <div class="flex gap-4"> <a href="/">主页</a> <a href="/posts/page/1">所有文章</a> <a href="/search/">文章检索</a> </div>  <h1>Python的GC和引用计数</h1> <p>最近发现Python的GC机制非常地独树一帜。在常用的带GC的编程语言中，可能只有Python的GC机制采用了引用计数的机制。据说这种机制实现的GC效率并不是最高的，但是这种机制却意外地很适合神经网络的场景。</p> <p>在没有循环引用的情况下，Python的GC机制可以保证对象在不再需要的时候立刻被回收。这种机制非常适合PyTorch管理tensor占用的显存。只要tensor不再被使用，它所占用的显存就会被立刻释放。至于GC的效率倒不是那么重要，毕竟神经网络的性能瓶颈在于GPU而非CPU。并且在神经网络的场景下，一般tensor是不会出现循环引用的。</p> <p>参考资料：<a href="https://pytorch.org/blog/understanding-gpu-memory-2/">Understanding GPU Memory 2: Finding and Removing Reference Cycles</a></p> <blockquote> <p>Python will clean up non-cyclic objects immediately using reference counting. However objects in reference cycles are only cleaned up later by a cycle collector.</p> </blockquote> <p>这篇文章也讲解了如何分析循环引用导致的显存占用过多的问题。</p> <p>而Java的GC机制则是采用了标记-清除的机制。这种机制不能保证对象在不再需要的时候立刻被回收，所以可能会导致显存的浪费。</p>  </body></html>