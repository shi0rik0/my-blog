<!DOCTYPE html><html> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><!-- KaTeX --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css" integrity="sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib" crossorigin="anonymous"><!-- Prism --><link href="/lib/prism.css" rel="stylesheet"><script src="/lib/prism.js"></script><meta name="generator" content="Astro v5.1.7"><title></title><link rel="stylesheet" href="/_astro/fee.D5xOGKbN.css"></head> <body class="bg-[#333] text-[#ddd]"> <h1>Sustie</h1> <div class="flex gap-4"> <a href="/">主页</a> <a href="/posts/page/1">所有文章</a> <a href="/search/">文章检索</a> </div>  <h1>用PyTorch进行混合精度训练/推理</h1> <p>最近尝试了用混合精度的方法来加速模型训练。要用PyTorch进行混合精度训练非常简单，因为PyTorch已经封装好了这个功能，API也很便捷，要import的东西只有下面这些：</p> <pre><code class="language-python">from torch.amp import autocast, GradScaler
</code></pre><p>要启用混合精度训练，对原有代码的改动也很少。首先是把前向传播的代码用<code>autocast</code>包裹起来：</p> <pre><code class="language-python">with autocast('cuda'):
    outputs = model(inputs)
    loss = criterion(outputs, targets)
</code></pre><p>然后在反向传播的时候，用<code>GradScaler</code>来缩放梯度：</p> <pre><code class="language-python">scaler = GradScaler()

# In the training loop:
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
optimizer.zero_grad()
</code></pre><h2>实验结果</h2> <p>我在一台配备RTX 4090的机器上进行了实验，发现对于GPT-2，混合精度训练确实能够大幅提升训练速度，但是对于普通的全连接层或者卷积层，提升效果并不明显。我的解释是：Transformer模型的计算量远大于全连接层或者卷积层，这样才能体现出混合精度训练的优势。</p>  </body></html>