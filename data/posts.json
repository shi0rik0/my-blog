[{"title":"日本ACGN资讯网站","date":"2025-01-23","url":"/posts/acgn-info","content":"制定去日本的旅行计划的时候，不妨参考一下这些网站。\n联动\nコラボカフェ\n演出等活动\nアニメ・声優イベントカレンダー - アニメハック\n同人展\n同人イベントNAVI\n电影\n映画.com\n","tags":["ACGN"]},{"title":"数据分析中的association问题及其算法","date":"2024-07-07","url":"/posts/association-algorithm","content":"问题定义\n顾客的交易记录可以用一个m×nm \\times nm×n的矩阵来表示，如果第iii个顾客购买了第jjj个商品，矩阵的第iii行第jjj列就为 1，否则为 0。\n接下来定义 3 个新概念：association、support 和 confidence。\n一个 association 可以用X→YX \\to YX→Y来表示，其中XXX和YYY都是商品的集合，X∩Y=∅X \\cap Y = \\emptysetX∩Y=∅。\n一个商品集合的 support 是指交易记录中包含这些商品的交易的个数。一个 association 的 support 是指X∪YX \\cup YX∪Y的 support。\n一个 association 的 confidence 是指X∪YX \\cup YX∪Y的 support 除以XXX的 support。\n所谓 association 问题就是，给定一个交易记录和两个常数K1K_1K1​和K2K_2K2​，希望找到所有的 association，满足其 support ≥K1\\ge K_1≥K1​且 confidence ≥K2\\ge K_2≥K2​。\n算法框架\n总的算法可以分成两步：第一步是找到所有满足 support ≥K1\\ge K_1≥K1​的商品集合，第二步是在第一步的基础上再找出满足条件的 association。\n这个过程中最困难的是第一步，因为可以证明第一步是 NP-hard 的。\n第二步的算法\n假设我们已经找到了 support ≥K1\\ge K_1≥K1​的商品集合的集合S1S_1S1​，那么最终结果S2S_2S2​可以表示为：\n\nS_2 = \\{ X \\to Y \\backslash X | X, Y \\in S_1 \\land X \\subsetneq Y \\land \\\\ \\text{confidence}(X \\to Y \\backslash X) \\ge K_2 \\}\n\n也就是说，枚举S1S_1S1​中的任意两个商品集合XXX和YYY，如果X⊊YX \\subsetneq YX⊊Y且X→Y\\XX \\to Y \\backslash XX→Y\\X的 confidence ≥K2\\ge K_2≥K2​，那么就将X→Y\\XX \\to Y \\backslash XX→Y\\X添加到最终结果的集合中。\n第一步的算法：Apriori 算法\n假设有NNN个商品，把我们最终要得到的 support ≥K1\\ge K_1≥K1​的商品集合的集合记作LLL（large 的意思）。Apriori 算法的思路是把LLL分拆成NNN个集合L1,L2,⋯ ,LNL_1, L_2, \\cdots, L_NL1​,L2​,⋯,LN​，其中LiL_iLi​是LLL中元素个数为iii的商品集合的集合。Apriori 算法会依次寻找L1,L2,⋯L_1, L_2, \\cdotsL1​,L2​,⋯，直到LNL_NLN​为止。\n要想求出L1L_1L1​是非常容易的，因此接下来只需要解决如何根据LiL_iLi​求出Li+1L_{i+1}Li+1​。当然我们可以直接用暴力枚举的方法求Li+1L_{i+1}Li+1​，但是这样速度太慢。我们需要考虑如何利用LiL_iLi​更快地求出Li+1L_{i+1}Li+1​。\nApriori 算法的核心是利用了以下性质：如果X⊊YX \\subsetneq YX⊊Y，那么support(X)>support(Y)\\text{support}(X) > \\text{support}(Y)support(X)>support(Y)。根据这个性质，如果X∈LX \\in LX∈L，那么其任意子集也必定 ∈L\\in L∈L。\n因此，求Li+1L_{i+1}Li+1​之前，可以先根据这个必要条件筛选掉一些不可能的商品集合。将筛选后的集合记作Ci+1C_{i+1}Ci+1​（candidate 的意思），则有\n\nC_{i+1} = \\{X|\\forall a \\in X, X\\backslash {a} \\in L_i \\}\n\n上面的公式只是数学描述，实际算法实现可以这么做：首先将LiL_iLi​中的每个商品集合中的商品按照一定顺序排序。然后枚举X,Y∈LiX, Y\\in L_iX,Y∈Li​，若X,YX, YX,Y只有最后一个商品不同，其他都相同，则检查X∪YX \\cup YX∪Y是否满足上述必要条件，若满足则加入Ci+1C_{i+1}Ci+1​中。\n求出Ci+1C_{i+1}Ci+1​之后，还要再查询交易记录，看看Ci+1C_{i+1}Ci+1​中哪些是真的满足 support ≥K1\\ge K_1≥K1​的，就可以求出Li+1L_{i+1}Li+1​了。\n除了上面介绍的 Apriori 算法，还有 FP-Growth 等经典的算法。\n","tags":[]},{"title":"Attention层的简单讲解","date":"2024-10-02","url":"/posts/attention-layer","content":"输入与输出的维度\nAttention 层是用来处理序列输入的，所以它接受的输入是一个二维的n×Nin \\times N_in×Ni​的矩阵，而输出则是一个n×Non \\times N_on×No​的矩阵。nnn是输入序列的 token 个数，NiN_iNi​则是 embedding 向量的长度，NoN_oNo​则是输出的状态向量的长度。理论上 Attention 层对于nnn没有限制，而NiN_iNi​和NoN_oNo​则是模型的超参数。\nAttention 层的超参数\n除了NiN_iNi​和NoN_oNo​以外，Attention 层还有其他超参数。\n一个 Attention 层包含 3 个全连接层，分别叫做 Query、Key 和 Value，用符号QQQ、KKK和VVV表示。这 3 个全连接层的输入维度都是NiN_iNi​，而输出维度都是超参数，分别为DqD_qDq​、DkD_kDk​和DvD_vDv​。稍后会看到，Attention 层要求Dq=DkD_q=D_kDq​=Dk​以及Dv=NoD_v=N_oDv​=No​。\n综上所述，Attention 层的超参数有NiN_iNi​、NoN_oNo​和DqD_qDq​。\n推理过程\n我们将输入的向量依次记作x1x_1x1​、x2x_2x2​……xnx_nxn​，同理输出的向量记作y1y_1y1​、y2y_2y2​……yny_nyn​。\n推理的时候，首先要将输入向量都经过全连接层一次，得到Q(x1)Q(x_1)Q(x1​)、K(x1)K(x_1)K(x1​)、V(x1)V(x_1)V(x1​)等等。最终的输出是由VVV的结果加权求和得到的：\n\ny_i = \\sum_j \\alpha_{ij} V(x_j), \\forall i\n\n因为αij\\alpha_{ij}αij​是权重，所以自然要满足：\n\n\\sum_j \\alpha_{ij} = 1,\\forall i\n\nαij\\alpha_{ij}αij​是根据QQQ和KKK的结果得到的：\n\n\\alpha^\\prime_{ij} = \\frac{Q(x_i) \\cdot K(x_j)}{\\sqrt{D_q}}, \\forall i, j\n\n这里我用的符号是α′\\alpha^\\primeα′而非α\\alphaα，因为α′\\alpha^\\primeα′不满足权重和为 1 的要求，因此不是最终结果。要让和为 1，使用 softmax 层处理一下即可。\n（计算α\\alphaα要除以Dq\\sqrt{D_q}Dq​​是为了归一化。）\n","tags":[]},{"title":"给boto3增加type hints","date":"2024-11-15","url":"/posts/boto3-type-hints","content":"最近使用boto3的时候，发现boto3.client()的返回值没有标注类型，导致开发的时候很不方便。我原先以为是boto3的开发者忘了添加，后来看了源代码才知道，boto3.client()返回的对象的 class 是在运行时动态生成的，所以很难进行 type hints。（这就是动态语言的坏处啊！）\n好在有个开发者生成了boto3各种 client 的 stub，包名叫boto3-stubs。假如使用的是 S3 client，就输入：\npip install boto3-stubs[s3]\n然后在 VSCode 里输入：\nclient = boto3.client('s3')\n应该就会看到client已经被自动推断出了类型。\n假如我们需要给接受 S3 client 的函数参数标注类型，就要这么写：\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from mypy_boto3_s3.client import S3Client\nelse:\n    S3Client = object\n\ndef f(x: S3Client):\n    pass\nTYPE_CHECKING是一个特殊常量，对于代码分析器，它的值为 True，正常运行时则为 False。加上S3Client = object的作用则是为了保证运行时S3Client依然是被定义的变量，不然执行时会报错。这个问题也可以通过加上from __future__ import annotations来解决。\n","tags":[]},{"title":"关于C++编译器的警告开关","date":"2024-12-28","url":"/posts/c-compiler-warnings","content":"C++编译器（主要是 GCC 和 Clang）都会提供一些控制警告的编译选项，这些选项的格式通常是-W<name>或者-Wno-<name>，其中<name>是警告的名称。例如-Wuninitialized表示启用“未初始化变量”的警告，-Wno-unused-variable表示禁用“未使用变量”的警告。\n由于警告有很多种类，逐一启用会非常麻烦，因此编译器也提供了一些选项来批量启动警报，其中一个很有误导性的选项是-Wall。很多人会误以为这个选项会启用所有警告，但实际上这只会启用一部分警报。如果想要启用更多警告，可以再添加-Wextra选项。但是就算使用-Wall -Wextra也仍然不会启用所有警告。\n如果你有洁癖，想要启用所有警告，可以使用-Weverything选项，但是这个选项只有 Clang 支持，GCC 不支持。并且，由于有些警告过于严格或者迂腐了，所以启用所有警告或许并不是一个好主意。\n我个人的习惯是使用-Wall -Wextra，我觉得这是一个比较折中的选择，并且 Clang 和 GCC 都支持这些选项。\n参考资料\nhttps://embeddedartistry.com/blog/2017/06/07/warnings-weverything-and-the-kitchen-sink/\n","tags":[]},{"title":"关于clangd的配置文件","date":"2024-12-18","url":"/posts/clangd-config","content":"clangd 是一个可以分析 C++代码，并帮助编辑器进行代码补全、跳转等功能的工具，但是 clangd 不会自动分析项目的 include 路径、编译选项等信息，这些信息需要通过配置文件来提供。clangd 支持两种配置文件，第一种是compile_commands.json，这个文件会给项目中的每一个源文件分别指定编译选项，通常这个文件是由程序自动生成的。第二种则是compile_flags.txt，这个文件会给项目中的所有文件指定相同的编译选项，这个文件往往是手动编写的。\n配置文件的查找路径\nclangd 的文档写得不是很清楚，根据我所了解的信息，clangd 查找配置文件的逻辑大概是这样的：\n如果使用的是compile_flags.txt，那么 clangd 只会在项目根目录下查找这个文件。如果使用的是compile_commands.json，那么 clangd 会从当前编辑的源文件所在的目录开始，逐级向上查找compile_commands.json，并且途中也会在每一个目录的build/目录下查找。举个例子，假如当前编辑的文件是/app/src/foo.cpp，那么 clangd 会依次在以下目录查找：/app/src/、/app/src/build/、/app/、/app/build/……。目前似乎没有办法手动指定配置文件的路径。\ncompile_flags.txt 的格式\ncompile_flags.txt的格式非常简单，只需要在每一行写一个clang的编译选项即可。例如：\n-I/usr/include\n-std=c++17\n-Wall\n-Wextra\n使用 CMake 生成 compile_commands.json\n要使用 CMake 生成compile_commands.json，只需要在CMakeLists.txt中启用该功能即可：\nset(CMAKE_EXPORT_COMPILE_COMMANDS ON)\n这样在构建项目的时候，CMake 就会在生成目录（通常是build/）下面生成compile_commands.json文件。\n使用 Bear 生成 compile_commands.json\n对于使用传统的 Makefile 构建的项目，可以使用Bear 工具来生成compile_commands.json。在 Ubuntu 下，Bear 可以通过 APT 安装，包名为bear。\n","tags":[]},{"title":"关于CSRF攻击及其防御","date":"2024-10-25","url":"/posts/csrf-attack","content":"什么是CSRF攻击\nCSRF 攻击是利用了浏览器的一个安全漏洞。假设用户在 a.com 上登入，浏览器获得了 cookie，接下来如果用户访问 b.com，而 b.com 访问了 a.com，则浏览器会将 cookie 附加在请求上，从而可能在用户不知情的情况下以登入权限进行了 a.com 的操作。尽管较新的浏览器修复了部分漏洞，但是由于要保持互联网的兼容性，因此并没有完全修复这个问题，并且我们不能假设用户使用的都是较新的浏览器，因此对 CSRF 攻击进行防御还是有必要的。\n如何防御CSRF攻击\n浏览器发起请求的两种分类\n要防御 CSRF 攻击，首先要知道在什么情况下，访问 b.com 的时候浏览器会对 a.com 发起请求。主要有两种情况：\n\n\n浏览器内置的行为：b.com的页面中有一个<img>标签，src属性指向a.com，这样浏览器会向a.com发起请求，或者有一个<form>表单，action属性指向a.com，这样如果用户提交了表单，则浏览器会向a.com发起请求。可能还有其他情况，这里只列出了最常见的情况。\n\n\nJavaScript脚本发起的请求：b.com执行了一个JavaScript脚本，脚本中向a.com发起了请求。\n\n\n对于第二种情况，这些请求会受到CORS机制的限制，而第一种情况则不会。这里我们还要补充一个概念：简单请求（Simple Request，其定义参见这里）。简单来说，所有第一种情况下可能发出的请求都是简单请求。\n然后我们还要解释一下CORS机制的一个细节：CORS 机制对于简单请求和复杂请求的处理是不同的。对于复杂请求，它会先给服务器发送一个 OPTIONS 请求，只有服务器返回一个允许的响应，浏览器才会发起真正的请求。而对于简单请求，浏览器会直接发送真正的请求，只不过如果服务器的响应没有说允许请求，那么浏览器将不会允许脚本获得响应的值。此时服务器还是收到了请求。这意味着什么呢？设想一下，如果一个 API 的功能是修改密码，那么只要服务器执行了该请求，攻击就已经成功了，这与浏览器脚本能否获得响应值无关。\n防御方法\n最简单的防御方法自然是完全不使用Cookie来认证，但是这样未免有点因噎废食。还有一种常用的方法是利用CSRF token，但是这种方法不适合于前后端分离的架构。这里我要介绍一种更简单的防御方法：添加自定义请求header。\n我们只需要自定义一个请求header，例如X-CSRF-TOKEN，然后在服务端拒绝所有不包含这个header的请求即可（当然OPTIONS请求还是要按正常逻辑处理）。这样一来就拒绝了所有简单请求，而复杂请求只可能是由JavaScript脚本发起的，受到CORS的限制会先发送一个OPTIONS请求，这样就保证了只有同源的情况下才能发起真正的请求。\n参考资料\n\n\nhttps://developer.mozilla.org/en-US/docs/Web/HTTP/CORS\n\n\nhttps://owasp.org/www-community/attacks/csrf\n\n\nhttps://danielw.cn/web-security-xss-csrf-cn#csrf\n\n\nhttps://security.stackexchange.com/a/58308\n\n\n","tags":[]},{"title":"关于在data warehouse中实例化view的优化问题","date":"2024-07-14","url":"/posts/data-warehouse-problem","content":"问题背景\n假设我们有一个名为 SalesTable 的 SQL 表格，这个表格有 4 列：time_、kind、shop 和 sales，分别代表某个时间段内某个种类的商品在某家商店的销售额。\n假如我们需要统计每个时间段内每个种类商品的销售总额，那么我们可以这么编写 SQL 语句：\nSELECT time_, kind, SUM(sales) FROM SalesTable GROUP BY time_, kind;\n我们不妨将这个查询结果记作 TK。\n假如我们还需要统计每个种类商品的销售总额，那么我们可以这么编写 SQL 语句：\nSELECT kind, SUM(sales) FROM SalesTable GROUP BY kind;\n我们将这个查询结果记作 K。要想得到 K，还有另外一种方法，就是利用前面的 TK：\nSELECT kind, SUM(sales) FROM TK GROUP BY kind;\n假如我们将 TK 的结果实例化（materialize，也就是缓存），那么就可以减少查询 K 的开销，因为 TK 的行数显然比 SalesTable 少得多。\n问题定义\n给定一个有向图(G,E)(G,E)(G,E)，图中的每个节点代表一个表格或者 view，如果节点BBB可以由节点AAA得到，就建立一条边A→BA \\to BA→B。我们可以实例化一些节点，把实例化的节点集合记作MMM。把一个节点的“邻居”和它自身记作N(V)N(V)N(V)，也就是说，N(V)={V}∪{W∣W→V∈E}N(V)=\\{V\\} \\cup \\{W| W \\to V \\in E\\}N(V)={V}∪{W∣W→V∈E}。把一个节点的行数（数据规模）记作S(V)S(V)S(V)。一个节点的查询代价定义为\n\nC(V) = \\min_{W\\in N(V) \\cap M} S(W)\n\n现在的问题是，我们已经实例化了一些节点（通常是根表格，比如说上面例子中的 SalesTable），我们希望再实例化KKK个节点（KKK是一个常数），使得总查询代价C=∑V∈GC(V)C=\\sum_{V\\in G}C(V)C=∑V∈G​C(V)能够减少最多。我们将这个问题的结果定义为这个减少量。\n贪心算法\n我们可以采用贪心算法解决这个问题，也就是说，我们依次选取节点加入MMM中，每次都选取一个能使CCC减少最多的节点。\nGupta [1] 证明了这么一个结论：贪心算法的近似比是1−1e≈0.631-\\frac{1}{e} \\approx 0.631−e1​≈0.63。也就是说，贪心算法得到的结果至少是最优结果的 0.63 倍。\n参考资料\n[1] Gupta, H. (1997). Selection of views to materialize in a data warehouse. In Database Theory—ICDT'97: 6th International Conference Delphi, Greece, January 8–10, 1997 Proceedings 6 (pp. 98-112). Springer Berlin Heidelberg.\n","tags":[]},{"title":"3种决策树：ID3、C4.5和CART","date":"2024-07-07","url":"/posts/decision-tree-id3-c45-cart","content":"假设有以下数据集，记录着一些人的种族、收入、是否有子女以及他们是否有购买保险：\n\n\n\nRace\nIncome\nChild\nInsurance\n\n\n\n\nB\nH\nN\nY\n\n\nW\nH\nY\nY\n\n\nW\nL\nY\nY\n\n\nW\nL\nY\nY\n\n\nB\nL\nN\nN\n\n\nB\nL\nN\nN\n\n\nB\nL\nN\nN\n\n\nW\nL\nN\nN\n\n\n\n我们希望构造一棵决策树，来根据种族、收入、是否有子女这三个属性来预测一个人是否有购买保险。构造决策树的关键问题在于决定分叉处要根据哪个属性来进行分类。常见的选择方法有 3 种：ID3、C4.5 和 CART。\n在下文中，将属性集{Race,Income,Child}\\{\\text{Race}, \\text{Income}, \\text{Child}\\}{Race,Income,Child}记作DDD。将要预测的目标 Insurance 记作TTT。\nID3\nID3 选择属性的方法是：\n\n\\argmax_{X\\in D} H(T) - H(T|X) = \\argmin_{X\\in D} H(T|X)\n\nC4.5\nC4.5 选择属性的方法是：\n\n\\argmax_{X\\in D} \\frac{H(T) - H(T|X)}{H(X)}\n\nCART\nCART 选择属性的方法是：\n\n\\argmax_{X \\in D} G(T) - G(T|X) = \\argmin_{X \\in D} G(T|X)\n\n其中G(X)G(X)G(X)是 Gini 指数，其定义为：\n\nG(X) = 1- \\sum_{x} p^2(x)\n\n例如G(T)=1−(12)2−(12)2=12G(T) = 1-(\\frac{1}{2})^2-(\\frac{1}{2})^2=\\frac{1}{2}G(T)=1−(21​)2−(21​)2=21​。\n条件 Gini 指数的定义和条件信息熵类似：\n\nG(X|Y)=\\sum_y p(y)G(X|Y=y)\n\n","tags":[]},{"title":"Windows家庭版关闭BitLocker","date":"2024-07-20","url":"/posts/disable-bitlocker","content":"现在很多笔记本出厂时会默认启用 BitLocker。通过运行manage-bde -status命令可以查看分区是否有启用 BitLocker（这个命令需要管理员权限）。\n如果是家庭版的 Windows（可以通过运行winver确认），那么 BitLocker 可以用系统设置的“隐私和安全性”->“设备加密”进行设置。这个“设备加密”就是家庭版 Windows 自带的阉割版 BitLocker。关闭“设备加密”就可以解密了。解密完成后，可以通过运行manage-bde -status命令确认是否已经解密成功。\n","tags":[]},{"title":"Linux下禁用IOMMU的方法","date":"2025-03-10","url":"/posts/disable-iommu","content":"最近在安装Isaac Sim的时候，遇到了提醒我关闭IOMMU的警告。下面总结一下在Linux系统中关闭IOMMU的方法。\n要关闭IOMMU，最简单的办法是修改内核的启动参数。首先用文本编辑器打开/etc/default/grub，找到GRUB_CMDLINE_LINUX_DEFAULT=\"...\"这一行（如果没有就添加这一行），然后在引号中添加以下内容：\nGRUB_CMDLINE_LINUX_DEFAULT=\"intel_iommu=off amd_iommu=off\"\n保存文件后，运行以下命令更新grub配置：\nsudo update-grub\n然后重启系统就可以了。\n","tags":[]},{"title":"Django外键的on_delete参数","date":"2024-11-11","url":"/posts/django-on-delete","content":"在 Django 的 ORM 框架中，通过models.ForeignKey可以很方便地创建外键字段。请看下面的例子：\nfrom django.db import models\n\nclass Manufacturer(models.Model):\n    name = models.TextField()\n\nclass Car(models.Model):\n    manufacturer = models.ForeignKey(Manufacturer, on_delete=models.PROTECT)\n在创建外键的时候，必须指定on_delete参数。在早期的 Django 版本中，这个参数有一个默认值CASCADE，而这个选项是非常危险的。假如数据库中有一些 Car 的 Manufacturer 是 BMW，那么如果设置了CASCADE，删除 BMW 就会将由 BMW 生产的所有 Car 的记录也一并删除，这样很可能导致数据丢失。如果设置为PROTECT，就会拒绝这次删除操作，并抛出一个异常。事实上，在生产环境中，往往很少直接删除记录，最多就是用一个专门的deleted字段来进行伪删除，因此将on_delete设置为PROTECT是我心目中的最佳实践。（on_delete还有一些安全性介于CASCADE和PROTECT之间的选项，例如SET_NULL和RESTRICT等等，这里就不赘述了。）\n","tags":[]},{"title":"域名的DNS设置","date":"2024-10-30","url":"/posts/dns-settings","content":"当我们购买了一个域名之后，必须要做的一件事就是将域名绑定到服务器的 IP 地址上。这个过程是通过在域名注册商那里修改 DNS 设置实现的。当我们在域名注册商那里购买一个域名之后，我们就可以通过域名注册商的控制面板来管理域名的 DNS 设置。\n指定 DNS 服务器\n通常来说，域名注册商会有自己的 DNS 服务器。默认情况下，域名会使用域名注册商自己的 DNS 服务器，但我们也可以改为第三方的 DNS 服务器。下图是 GoDaddy 的配置页面，图中我选择使用 CloudFlare 的 DNS 服务器。\n\n修改 DNS 记录\n假如我们选择使用域名注册商的 DNS 服务器，那么修改 DNS 记录就是在域名注册商的控制面板上进行。如果选用第三方的 DNS 服务器，那么配置就是在第三方服务商的控制面板上进行。\n假设我们购买的域名是 foo.com。一个典型的 DNS 记录表如下所示：\n\n\n\nType\nName\nData\n\n\n\n\nA\n@\n11.4.5.14\n\n\nCNAME\nwww\nfoo.com\n\n\n\n首先是最核心的 A 记录。Name 的@可以理解为是空值的意思。第一行的 A 记录的意思是：foo.com 的 IP 地址是 11.4.5.14。\n然后是 CNAME 记录。第二行的 CNAME 记录的含义是：www.foo.com的IP地址等于foo.com的IP地址。\n通过这两行记录，foo.com 和www.foo.com的IP地址都会被设定为11.4.5.14。\n","tags":[]},{"title":"docker compose up的build选项","date":"2024-10-02","url":"/posts/docker-compose-build","content":"从本地的 Dockerfile 构建镜像启动 Docker Compose 的时候（比如像下面的配置），\nservices:\n  service0:\n    build:\n      context: .\n      dockerfile: Dockerfile\n一定要记得在docker compose up的时候加上--build的选项：\ndocker compose up --build\n因为 Docker Compose 的缓存机制非常粗糙，它不会检查 Dockerfile 是否有更改，也不会检查构建过程中是否有缓存失效了，所以 99%的情况下不添加--build会导致修改无法生效。\n总结：使用docker compose up的时候一定要加上--build的选项。\n","tags":[]},{"title":"关于Docker的当前目录","date":"2024-09-20","url":"/posts/docker-current-dir","content":"最近被 Docker 的当前目录位置整得有些混乱，特地在此整理一下。\nDocker Build\n执行docker build命令的时候，需要传入一个 positional 参数，这个参数就是 context。举个例子，假如当前 shell 的目录是/root，我们执行以下命令：\ndocker build -f dockerfiles/a.Dockerfile dockerfiles\n那么，构建过程中 Dockerfile 的 context 就是/root/dockerfiles。Context 的值会有以下三个影响：\n\nContext 的值就是 Dockerfile 中.所代表的目录。\nDocker 会将 context 目录下的所有文件（除了被.dockerignore 排除的）复制一遍。在构建镜像的时候，COPY命令只能访问到 context 下的文件。这里有一个坑点：假如我们将 context 设置成/root，那么在 Dockerfile 中访问../tmp/a.txt的时候不会立刻报错，而是会访问/root/tmp/a.txt。\nDocker 在查找全局的.dockerignore 的时候，也是会在 context 目录下查找。但是这里又有一个坑点：Docker 还支持针对某个特定 Dockerfile 的.dockerignore，假设 Dockerfile 文件名为foo.Dockerfile，那么对应的.dockerignore 文件名就是foo.Dockerfile.dockerignore，但是这种.dockerignore 是在 Dockerfile 文件所在的目录下查找，而非 context 目录。举例来说，假如 Dockerfile 是/root/dockerfiles/a.Dockerfile，context 设置为/root，那么生效的.dockerignore 路径就是/root/.dockerignore以及/root/dockerfiles/a.Dockerfile.dockerignore。\n\nDocker Compose\nDocker Compose 的当前目录是docker-compose.yml配置文件所在的目录，而不是执行docker compose命令时 shell 的当前目录，并且没有选项可以更改它。\n但是这里又有一个坑点：在 Docker Compose 的配置文件中可以指定从本地的 Dockerfile 构建镜像，例如：\nservices:\n  service-0:\n    build:\n      context: ..\n      dockerfile: docker/Dockerfile\n这里面的context选项是相对于docker-compose.yml所在目录的，但是dockerfile选项却是相对于context的。也就是说，假如上面的配置文件位于/foo/docker-compose.yml，那么context就是/，而 Dockerfile 应该位于/docker/Dockerfile。\n","tags":[]},{"title":"Docker Compose确保Postgres启动的方法","date":"2024-10-16","url":"/posts/docker-healthcheck","content":"最近我编写了一个 Django + Postgres 的网站，其 docker-compose.yml 大概是下面这样的：\nservices:\n  db:\n    image: postgres\n\n  web:\n    image: django\n    depends_on:\n      - db\n根据 Docker 文档，指定depends_on属性可以确保 Django 容器在 Postgres 容器启动之后才启动。但是这么设置之后，会发现启动的时候 Django 还是有可能连接不上 Postgres 导致异常。其原因在于：Postgres 容器启动（也就是 Postgres 进程启动）不代表 Postgres 进程已经准备好接受连接了。这之间还需要一段时间用来初始化。因此上面的 docker-compose.yml 是不可靠的。\n正确的写法应该是下面这样的：\nservices:\n  db:\n    image: postgres\n    healthcheck:\n      test: pg_isready || exit 1\n      interval: 5s\n      timeout: 10s\n      retries: 120\n\n  web:\n    image: django\n    depends_on:\n      db:\n        condition: service_healthy\n首先看db的healthcheck属性。这个属性用来启用 Docker 的 healthcheck 机制。Docker 会定期在容器内运行test所指定的命令，根据命令的返回值来判断容器的健康状态。pg_isready是 Postgres 官方提供的命令，用于检测 Postgres 是否已经完全启动。\n这里有一个乍看之下比较奇怪的地方，就是在命令后面还加上了|| exit 1。这是因为，根据Docker 官方文档，如果test所指定的命令的结束代码为 0，则表示容器健康；若为 1，则表示容器异常；结束代码 2 则作为保留值，不应该使用它（吐槽一下：那么除了 0、1、2 以外的结束代码呢？）。在 Shell 脚本中，a || b表示仅当a的结束代码为 0 时才执行b。因此当pg_isready返回 0 的时候，pg_isready || exit 1的结束代码是 0；若pg_isready返回非 0 的值，则pg_isready || exit 1会一律返回 1。这样就符合了 Docker 的约定。\n然后在web的depends_on属性里加入condition: service_healthy，就可以让web容器在db容器处于 healthy 状态后才启动。\n","tags":[]},{"title":"Windows下Docker对路径斜杠处理的一个bug","date":"2024-10-22","url":"/posts/docker-windows-slash-bug","content":"最近在 Windows 下使用 Docker 的时候遇到了一个 bug。起因是我需要将 host 上的一个文件挂载到 Docker 容器里，于是我就执行了以下命令：\ndocker run -v ./foo.txt:/tmp/foo.txt ubuntu\n结果在容器里却无法打开foo.txt，后来发现 Docker 居然在容器里创建了一个空的名为foo.txt的目录！\n后来我发现以下两种方法都可以解决问题：\n\n使用绝对路径。\n将斜杠改为 Windows 使用的反斜杠。\n\n我推测这是因为 Docker 在 Windows 下对于路径斜杠处理存在 bug。\n","tags":[]},{"title":"Dockerfile中CMD和ENTRYPOINT的区别","date":"2024-10-14","url":"/posts/dockerfile-cmd-vs-entrypoint","content":"在 Dockerfile 中有两个命令CMD和ENTRYPOINT，它们都可以用来指定 Docker 镜像启动时的默认命令，那么它们究竟有什么区别呢？\n我在网上看到一篇博客文章很好地解释了两者的区别。简而言之，最终运行docker run命令的时候，容器执行的命令会是 ENTRYPOINT + CMD，但是有以下几点需要注意：\n\n如果 ENTRYPOINT 是字符串形式而非数组形式，那么 CMD 会被忽略。\nDockerfile 中指定的 ENTRYPOINT 和 CMD 都可以在执行docker run命令的时候被修改。命令格式为：docker run --entrypoint <ENTRYPOINT> <image> <CMD>。\n\n我引用的博客文章的作者推荐使用 ENTRYPOINT，但我习惯只使用 CMD，而不设置 ENTRYPOINT，原因是这样比较简单，也不容易出错。\n","tags":[]},{"title":"关于.dockerignore的陷阱","date":"2024-09-24","url":"/posts/dockerignore-trap","content":"在 Docker 构建镜像的时候，默认会将 context 下的所有文件打包发送到 builder 进程。这是因为 Docker builder 采用了一种客户端/服务器的架构，这样就可以实现远程构建。但是如果 context 下面文件很大，就会影响构建速度。为此，Docker 提供了一个机制，可以通过在.dockerignore文件中指定要排除的文件。但是这个.dockerignore 的语法和.gitignore 有很大差异，因此很容易写错，其中一个陷阱就是，在.gitignore 中，foo等价于**/foo，而在.dockerignore 中，这等价于/foo。除此之外还有很多微妙的差异，比如这篇文章中指出的：\n\nIn .gitignore, if a path is listed as ignored (e.g. /dir), then any files or folders under that path is ignored and it is not possible to re-include them via negation (e.g. !/dir/file) as the parent directory of them is excluded;\nwhile in .dockerignore, even if a parent path is ignored, files or folders under it can still be re-included via negation.\n\n","tags":[]},{"title":"Electron IPC教程","date":"2025-02-10","url":"/posts/electron-ipc","content":"Electron的进程模型\n在Electron中，有两种进程：主进程和渲染进程。渲染进程本质上就是一个Chromium的进程，它只负责渲染页面，不能访问Node.js提供的操作系统相关的API。主进程是Node.js的进程，可以访问操作系统的API。主进程和渲染进程之间通过IPC（Inter-Process Communication）进行通信。\n这种模型的好处就在于：\n\n不需要大幅修改Chromium的源码。\n保证了浏览器环境的安全性。\n\n如何使用IPC\n假设我们希望在渲染进程中读取本地文件。\n首先在主进程的入口main.ts中创建一个接口read-file：\nimport { app, BrowserWindow, ipcMain } from 'electron'\nimport { promises as fs } from 'fs'\n\napp.on('ready', () => {\n  ipcMain.handle('read-file', async (event, filePath: string) => {\n    return await fs.readFile(filePath, 'utf-8')\n  })\n  createWindow()\n})\n这里要注意，不能在createWindow函数中注册API，因为createWindow函数可能会被调用多次。\n然后在preload.ts中将read-file接口暴露给渲染进程：\nimport { contextBridge, ipcRenderer } from 'electron'\n\ncontextBridge.exposeInMainWorld('eAPI', {\n  readFile: (filePath: string) => ipcRenderer.invoke('read-file', filePath),\n})\n（preload.ts是渲染进程在正式运行前执行的一个脚本，它具有更多的权限。这个设计主要是为了保证运行时的安全。）\n这样就可以在渲染进程中通过window.eAPI.readFile()来调用主进程的read-file接口了。例如在Vue组件中：\n<script setup lang=\"ts\">\nimport { ref } from 'vue'\n\nconst content = ref<string>('')\nconst readFile = async () => {\n  content.value = await window.eAPI.readFile('path/to/file')\n}\n</script>\n\n<template>\n  <button @click=\"readFile\">Read File</button>\n  <p>{{ content }}</p>\n</template>\nTypeScript支持\nElectron无法自动推导出IPC接口的类型，只能手动定义。在项目的任意位置创建一个.d.ts文件，例如src/window.d.ts，然后定义接口：\ninterface API {\n  readFile: (filePath: string) => Promise<string>\n}\n\ndeclare global {\n  interface Window {\n    eAPI: API\n  }\n}\n这样TypeScript就能正确推导出window.eAPI.readFile()的类型了。\n","tags":["electron"]},{"title":"在Electron中读取并显示本地图片","date":"2025-02-12","url":"/posts/electron-read-images","content":"最近需要在Electron应用程序中实现读取并显示本地图片的功能。这个功能看起来简单，但实现起来还是会遇到很多坑。这里就总结一下我遇到的问题，并给出一个完整的解决方案。\n两种方案\n首先，Electron是不支持在渲染进程中直接读取本地文件的，所以必须要通过主进程来读取文件。这里有两种方案：\n\n利用Electron的Protocol API，将本地文件映射到一个自定义的协议，然后在渲染进程中通过这个协议来读取文件。\n直接在主进程中提供一个读取文件的接口，然后在渲染进程中调用这个接口来读取文件。\n\n第一种方案看起来更优雅，因为这样就可以直接通过修改img标签的src属性来显示图片，但实际上这种方案会导致一个问题：修改src属性之后图片不会自动刷新，所以最终我放弃了这种方案。\n序列化问题\n如果采用第二种方案，就会遇到一个问题：如何在主进程和渲染进程之间传递文件数据。最直接的方法是将文件数据序列化为Base64字符串，但是这样势必会降低性能，所以我希望能够直接传递二进制数据。但是Electron的序列化机制似乎无法正确传递Buffer对象，所以要将Buffer对象转换为Uint8Array对象。\nipcMain.handle('load-file', async (_, filePath: string) => {\n  const content = await fs.readFile(filePath, { encoding: null })\n  return Uint8Array.from(content)\n})\n让img标签显示图片\n最后的问题就是如何让img标签显示二进制数据的图片。一种简单的办法是将二进制数据转换为Base64字符串，然后将这个字符串赋值给img标签的src属性，比如说<img src=\"data:image/png;base64,{base64_str}\">。但这里又有个麻烦的地方：我们必须指定图片的MIME类型。当然我们可以通过图片扩展名或者元数据来判断图片类型，但是我嫌麻烦，所以后来发现一个更简单的方法，直接指定MIME类型为image/unknown，这样浏览器会自动识别图片类型（当然这不是符合标准的）。\n","tags":["electron"]},{"title":"给VSCode Vue项目配置ESLint和Prettier","date":"2024-11-25","url":"/posts/eslint-prettier-for-vue-vscode","content":"什么是 ESLint 和 Prettier\nESLint 是一个专门针对 JS 程序的 linter，但是它也可以用来格式化 JS 代码。而 Prettier 是一个支持多种语言（包括 JS）的代码 formatter。Linter 和 formatter 的区别在于，formatter 只关注代码格式好不好看，不关心具体逻辑，而 linter 更关注程序逻辑是否有问题。\n所以说，对于 HTML 和 CSS 代码，只能用 Prettier 来格式化。而对于 JS 代码来说，ESLint 和 Prettier 的功能有重复的部分。比较好的做法是，禁用 ESLint 中与 Prettier 冲突的格式化规则，然后同时使用 ESLint 和 Prettier。\n安装并配置 Prettier\n在下文中，我们假设项目的目录结构是这样的：\n.\n├── backend/\n├── frontend/\n│   ├── node_modules/\n│   ├── package.json\n│   ├── package-lock.json\n首先我们要安装 Prettier：\nnpm install -D prettier\n然后在frontend/下面创建一个名为.prettierrc的 Prettier 配置文件。\n{\n  \"semi\": false,\n  \"singleQuote\": true,\n  \"tabWidth\": 2,\n  \"trailingComma\": \"es5\"\n}\n然后就可以用npx运行 Prettier 了：\nnpx prettier --check \"**/*.js\"\n安装并配置 ESLint\n首先安装依赖：\nnpm install -D eslint eslint-config-prettier eslint-plugin-vue\n其中eslint是 ESLint 本体，eslint-config-prettier是一个配置文件，用来禁用 ESLint 中与 Prettier 冲突的规则，eslint-plugin-vue则用来给 ESLint 添加 Vue 相关的规则。\n然后在frontend/下面创建名为eslint.config.js的 ESLint 配置文件：\nimport js from \"@eslint/js\";\nimport prettier from \"eslint-config-prettier\";\nimport pluginVue from \"eslint-plugin-vue\";\n\nexport default [\n  js.configs.recommended, // 使用推荐的 ESLint JS 配置\n  ...pluginVue.configs[\"flat/recommended\"], // 使用推荐的 ESLint Vue 配置\n  prettier, // 这个配置会禁用和 Prettier 有冲突的 ESLint 规则\n  {\n    rules: {\n      // 在这里写其他的配置\n      // 参考：https://eslint.org/docs/latest/use/configure/configuration-files\n    },\n  },\n];\n接着就可以使用 ESLint 了：\nnpx eslint \"**/*.js\"\n配置 VSCode\n首先安装以下 VSCode 插件：\n\ndbaeumer.vscode-eslint\nesbenp.prettier-vscode\nVue.volar\n\n然后打开工作区的配置文件.vscode/settings.json，在里面加入以下内容：\n{\n  \"editor.formatOnSave\": true,\n  \"editor.codeActionsOnSave\": {\n    \"source.fixAll.eslint\": \"explicit\"\n  },\n\n  \"prettier.configPath\": \"./frontend/.prettierrc\",\n  \"eslint.workingDirectories\": [\"./frontend/\"],\n\n  \"[vue]\": {\n    \"editor.defaultFormatter\": \"esbenp.prettier-vscode\"\n  },\n  \"[javascript]\": {\n    \"editor.defaultFormatter\": \"esbenp.prettier-vscode\"\n  },\n  \"[html]\": {\n    \"editor.defaultFormatter\": \"esbenp.prettier-vscode\"\n  },\n  \"[css]\": {\n    \"editor.defaultFormatter\": \"esbenp.prettier-vscode\"\n  }\n}\n","tags":[]},{"title":"使用pypdf库提取PDF文件中的图片","date":"2024-09-13","url":"/posts/extract-pdf-images","content":"最近我遇到了这么一个问题：我用扫描仪扫描出来的文件是 PDF 格式的，然而我希望得到的是图片格式。从理论上说，扫描仪生成的 PDF 文件应该也只是图片的集合，因此只需要解析 PDF 文件就可以提取了。有一个名为pypdf的 Python 库就可以解析 PDF 文件中的对象。\nfrom pypdf import PdfReader\n\nreader = PdfReader('test.pdf')\n\nfor p in reader.pages:\n    for i in p.images:\n        with open(i.name, 'wb') as f:\n            f.write(i.data)\n最后再补充说明一下 PDF 格式的大体原理。PDF 文件是由许多“对象”组成的。每个对象都有唯一的编号。可以通过编号来引用对象。对象有很多种，文字、图片乃至页都可以是对象。页对象里面记录着每一页有哪些对象，以及这些对象的位置、大小等。\n","tags":[]},{"title":"我最喜欢的clang-format配置","date":"2025-03-27","url":"/posts/favorite-clang-format","content":"我最喜欢的clang-format配置是：\nBasedOnStyle: WebKit\nclang-format提供了很多预定义的风格，其中缩进宽度为4个空格的只有WebKit和Microsoft风格，而我又不喜欢总是花括号另起一行，所以最后就只剩下WebKit风格了。\n","tags":[]},{"title":"一个无聊的问题：第一个版本号究竟是什么？","date":"2025-03-10","url":"/posts/first-version","content":"最近强迫症发作，突然想到一个很无聊的问题：软件的第一个版本应该是什么？0.0.1？0.1.0？还是0.0.0？\n这种情况我一般会选择随大流或者相信权威。语义化版本（Semantic Versioning）是一个被广泛采用的标准。在Semantic Versioning 2.0.0中，有这样一段话：\nHow should I deal with revisions in the 0.y.z initial development phase?\nThe simplest thing to do is start your initial development release at 0.1.0 and then increment the minor version for each subsequent release.\nHow do I know when to release 1.0.0?\nIf your software is being used in production, it should probably already be 1.0.0. If you have a stable API on which users have come to depend, you should be 1.0.0. If you’re worrying a lot about backward compatibility, you should probably already be 1.0.0.\n根据以上内容，一个软件的版本号最初应该是0.1.0，然后正式版的第一个版本号是1.0.0。\n","tags":[]},{"title":"在flex布局中实现按比例分配宽度或高度","date":"2025-01-08","url":"/posts/flex-ratio","content":"需求\n假设我们有这样的布局：\n<div class=\"flex flex-col w-[400px] h-[400px]\">\n  <div>div 1</div>\n  <div>div 2</div>\n  <div>div 3</div>\n</div>\n我们希望实现这样的效果：三个 div 的高度固定按照 1:2:3 的比例分配。\n解决方案\n<div class=\"flex flex-col w-[400px] h-[400px]\">\n  <div class=\"flex-1 min-h-0 overflow-clip\">div 1</div>\n  <div class=\"flex-[2_0_0] min-h-0 overflow-clip\">div 2</div>\n  <div class=\"flex-[3_0_0] min-h-0 overflow-clip\">div 3</div>\n</div>\n要点解析\nflex-grow 和 flex-basis\n在 Tailwind CSS 中，class flex-1 表示 CSS 中的属性 flex: 1 1 0%，flex-[2_0_0] 表示 flex: 2 0 0，flex-[3_0_0] 表示 flex: 3 0 0。而 CSS 中flex: X Y Z是一个缩写，相当于：\nflex-grow: X;\nflex-shrink: Y;\nflex-basis: Z;\n其中，flex-grow 表示元素的放大比例，flex-shrink 表示元素的缩小比例，flex-basis 表示元素的基础大小。\n下面举个例子来说明flex-grow和flex-basis的作用，flex-shrink的道理是类似的。\n考虑下面的布局：\n<div class=\"flex flex-col h-[400px]\">\n  <div class=\"flex-[2_0_100px]\">div 1</div>\n  <div class=\"flex-[1_0_0]\">div 2</div>\n</div>\n父元素的高度是 400px，而 div 1 的 basis 是 100px，div 2 的 basis 是 0。因此，减去基础高度之后，父元素还剩下 300px 的高度可以分配，这些空间按照 2:1 的比例分配给 div 1 和 div 2，所以 div 1 的分配到的高度是 200px，div 2 是 100px。最终的效果是 div 1 的高度是 100 + 200px = 300px，div 2 的高度是 0 + 100px = 100px。\nmin-h-0\n在解决方案中，我们还设置了min-h-0，也就是min-height: 0。这是因为在 flex 布局中，如果子元素的内容超出了子元素原本的高度，子元素会被强制拉伸，从而破坏比例分配。因此，我们需要设置min-height: 0来让溢出的内容不会影响到父元素的高度。\noverflow-clip\n经过上面的设置，现在 div block 的高度已经没问题了，但是 div 里面的内容还是可能会溢出。因为在 HTML 中，并没有规定元素的内容（比如文本）不能超出元素的边界。因此，我们需要设置overflow-clip来截断溢出的内容，也可以设置overflow-auto来显示一个滚动条。\n关于 padding 和 border 要注意的地方\n这里有一个细节，就是 padding 和 border 也会影响元素的基础宽度或高度，从而破坏比例分配。StackOverflow 上有一个专门讨论这个问题的帖子：Why is padding expanding a flex item?。\n简单来说，问题出在flex-basis的计算上。最终生效的flex-basis的值是按照下面的公式计算的：\nreal_flex_basis = max(flex_basis, padding + border)\n下面是一些解决办法：\n\n如果没有强迫症的话，就不管这个问题。反正一般情况下 padding 和 border 不会太大，不会对比例分配产生太大的影响。\n设置 padding 和 border 都为 0。\n如果需要设置 padding 或 border，可以手动将flex-basis设置成一个符合比例的同时又略微大于 padding + border 的值。\n\n","tags":[]},{"title":"关于Python的from __future__ import annotations","date":"2024-05-20","url":"/posts/from-future-import-annotations","content":"在默认情况下，Python 的 type hints 的值是真正的对象。请看下面的例子：\ndef add(a: int, b: int) -> int:\n    return a + b\n\n\nprint(add.__annotations__)\n\n# output:\n# {'a': <class 'int'>, 'b': <class 'int'>, 'return': <class 'int'>}\n但这么做会引发一些问题，比如说考虑下面的代码：\nclass Node:\n    left: Node\n    right: Node\n\n    def __init__(self):\n        pass\n这段代码是无法运行的，因为执行left: Node的时候，Node对象还没有被创建。\nfrom __future__ import annotations可以解决这个问题。一旦启用这个特性，type hints 存储的值就会变成一个字符串。只有调用typing.get_type_hints()之后才会解析字符串，返回真正的对象。请看下面的例子：\nfrom __future__ import annotations\nimport typing\n\n\ndef add(a: int, b: int) -> int:\n    return a + b\n\n\nprint(add.__annotations__)\nprint(typing.get_type_hints(add))\n\n# output:\n# {'a': 'int', 'b': 'int', 'return': 'int'}\n# {'a': <class 'int'>, 'b': <class 'int'>, 'return': <class 'int'>}\n由于这个特性改变了 Python 默认的行为，可能会导致依赖默认行为的库无法正常运行，因此 Python 将这个特性设计为可选的。\n顺带一提，启用这个特性还有一个好处，就是对内置容器类型（比如 list）进行 type hints 的时候可以直接写list[int]，而不用写typing.List[int]了。\n","tags":[]},{"title":"Python标准库的functools.wraps装饰器","date":"2024-11-04","url":"/posts/functools-wraps","content":"最近写代码的时候，发现给一个函数加了装饰器后就会报错。后来发现是因为装饰器抹去了原函数的 type hints 信息，导致后续逻辑出错。Python 的函数对象有很多 metadata，比如函数名、docstring、type hints 等，functools.wraps装饰器的作用就是复制这些 metadata。\n请看下面的示例：\nfrom functools import wraps\n\ndef decorator(func):\n    def new_func():\n        func()\n    return new_func\n\ndef decorator_with_wraps(func):\n    @wraps(func)\n    def new_func():\n        func()\n    return new_func\n\n@decorator\ndef f():\n    pass\n\n@decorator_with_wraps\ndef g():\n    pass\n\nprint(f.__name__)  # 输出：new_func\nprint(g.__name__)  # 输出：g\n","tags":[]},{"title":"常见编程语言的GC机制对比","date":"2025-04-02","url":"/posts/gc-comparison","content":"Java & C#\nJava和C#都采用了“分代”的思想，将对象分为不同的“代”，对于不同的“代”采用不同的GC策略。具体采用的GC策略包括：复制、标记-清除、标记-整理等。这种方式的优点是吞吐量高，缺点则是STW（Stop The World）时间较长。\nGo\nGo使用了三色标记法，这种方式的GC中断次数较多，但是每次的中断时间较短，可以说是一种牺牲吞吐量来换取较低STW时间的GC策略。\n参考资料：\n\nhttps://www.zhihu.com/question/326191221\nhttps://medium.com/servicetitan-engineering/go-vs-c-part-2-garbage-collection-9384677f86f1\n\nPython\nPython采取了引用计数和循环引用检测的双重机制。引用计数就和C++的shared_ptr类似，当对象的引用计数为0时就会被回收。引用计数无法处理循环引用的问题，所以需要定期进行循环引用检测。\nPython的GC机制非常地独树一帜。在常用的带GC的编程语言中，可能只有Python的GC机制采用了引用计数的机制。这种机制实现的GC效率并不是最高的，特别是STW时间较长，但是这种机制却很适合神经网络的场景。\n在没有循环引用的情况下，Python的GC机制可以保证对象在不再需要的时候立刻被回收。这种机制非常适合PyTorch管理tensor占用的显存。只要tensor不再被使用，它所占用的显存就会被立刻释放。至于GC的效率倒不是那么重要，毕竟神经网络的性能瓶颈在于GPU而非CPU。并且在神经网络的场景下，一般tensor是不会出现循环引用的。\n参考资料：Understanding GPU Memory 2: Finding and Removing Reference Cycles\n\nPython will clean up non-cyclic objects immediately using reference counting. However objects in reference cycles are only cleaned up later by a cycle collector.\n\n这篇文章也讲解了如何分析循环引用导致的显存占用过多的问题。\n","tags":[]},{"title":"使用date命令生成ISO 8601格式的当前时间","date":"2024-05-11","url":"/posts/generate-iso-8601","content":"在很多场合可以看到形如 2024-05-11T17:27:12+08:00 的时间字符串。这是用 ISO 8601 格式表示的时间。字符串中的“T”是日期和时间和分隔符，而“+08:00”是时间的偏移量，也就是说时区是东八区。\n在 Linux 下，可以使用 date 命令快速生成 ISO 8601 格式的当前时间：\ndate --iso-8601=seconds\n","tags":[]},{"title":"如何获取公网IP地址","date":"2025-03-18","url":"/posts/get-public-ip","content":"如果机器的网卡直接可以通过外网访问，那么可以直接通过以下命令获取网卡的IP地址：\nhostname -I\n如果机器的网卡不能直接通过外网访问，那么可以通过以下命令获取公网IP地址：\ncurl -x '' ifconfig.me\n-x ''的作用是禁用代理。\n","tags":[]},{"title":"Git取消追踪文件","date":"2025-01-19","url":"/posts/git-untrack","content":"Git 可以用.gitignore来忽略一些文件，但是如果是已经被追踪（track）的文件，将其添加到.gitignore并不会取消追踪。这时候就需要用到git rm --cached命令来手动取消追踪。但是有时候，误追踪的文件自己都不知道，有没有办法可以自动找出所有被追踪，但是原本应该被忽略的文件呢？\n查阅 Git 文档之后，发现有两个命令可以实现这个功能：\n\ngit ls-files，这个命令可以列出所有被追踪的文件。\ngit check-ignore --no-index，这个命令可以用来判断某个文件是否被.gitignore忽略。这里必须加上--no-index参数，否则将不会输出已经被追踪的文件。\n\n结合这两个命令，我们可以写一个脚本来找出所有被追踪，但是原本应该被忽略的文件：\nimport subprocess\n\n\ndef is_path_ignored(path: str) -> bool:\n    return subprocess.run(['git', 'check-ignore', '--no-index', path],\n                          check=True,\n                          capture_output=True).returncode == 0\n\n\ntracked_files = subprocess.run(['git', 'ls-files'],\n                               check=True,\n                               capture_output=True,\n                               text=True).stdout.splitlines()\n\nfor file in tracked_files:\n    if is_path_ignored(file):\n        print(file)\n","tags":[]},{"title":"使用GitHub Pages免费部署静态网站","date":"2025-03-03","url":"/posts/github-pages-tutorial","content":"要使用GitHub Pages服务部署静态网站，首先要创建一个GitHub仓库。默认情况下，如果你的仓库名字是<username>.github.io，那么你的网站将会被部署到https://<username>.github.io。否则，你的网站将会被部署到https://<username>.github.io/<repository-name>。举个例子，假如你的GitHub用户名是john，那么仓库john.github.io将会被部署到https://john.github.io，而仓库my-website将会被部署到https://john.github.io/my-website。当然，你也可以在设置中使用自定义域名。\n创建完仓库之后，要在仓库的设置中启用GitHub Pages（参见下图）。这里Source有两个选项：\"GitHub Actions\"和\"Deploy from a branch\"。如果选择后者，那么GitHub Pages会直接将指定分支的内容作为网站源代码。这个选项比较直观，我一般就用这个。\n\n如果你的网站是完全手写的，那么直接将网站内容上传到main分支，然后选择从main分支部署即可。但很多时候，网站是通过静态网站生成器生成的，这种情况下，如果手动上传生成的网站内容，就会比较麻烦。这时候，我们可以使用GitHub Actions来自动化这个过程。主要流程是：将生成网站的代码放在main分支下，然后通过GitHub Actions执行main分支，将生成的网站内容更新到gh-pages分支，最后选择从gh-pages分支部署网站。\n下面是我使用的一个GitHub Actions配置文件，它会在main分支有新的commit时，自动执行npm run build，然后将生成的dist目录下的网站内容推送到gh-pages分支。\nname: Deploy to GitHub Pages\n\non:\n  push:\n    branches:\n      - main\n  workflow_dispatch:\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    # Grant write permissions to GITHUB_TOKEN\n    permissions:\n      contents: write\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v2\n\n      - name: Set up Node.js\n        uses: actions/setup-node@v2\n        with:\n          node-version: '20'\n\n      - name: Install dependencies\n        run: npm install\n\n      - name: Build the project\n        run: npm run build\n\n      # Commit the generated files in the `dist` directory of the `main` branch\n      # to the `gh-pages` branch\n      - name: Deploy to GitHub Pages\n        uses: peaceiris/actions-gh-pages@v3\n        with:\n          github_token: ${{ secrets.GITHUB_TOKEN }}\n          publish_dir: ./dist\n","tags":[]},{"title":"使用Hugo + GitHub Pages搭建个人博客","date":"2024-03-17","url":"/posts/hugo-github-pages","content":"作为准备工作，你需要在电脑上安装 Hugo。具体的安装步骤请参照Hugo 官网。\n然后在 GitHub 上创建一个仓库，仓库名为<username>.github.io。例如我的 GitHub 用户名为 shi0rik0，就创建一个名为 shi0rik0.github.io 的仓库。\n然后打开仓库的\"Settings\"->\"Pages\"，将\"Source\"改为\"GitHub Actions\"。修改之后，你会看到下方有一些 GitHub 推荐的 workflow，这里选择将 Hugo 的 workflow 添加到仓库中。\n\n接下来执行以下命令。\ngit clone <your-repo>\ncd <your-repo>\nhugo new site . --force\ngit submodule add https://github.com/theNewDynamic/gohugo-theme-ananke.git themes/ananke\necho \"theme = 'ananke'\" >> hugo.toml\nhugo new content posts/my-first-post.md\n执行完之后，你就已经初始化了一个使用 ananke 主题的 Hugo 网站，并创建了第一篇博文posts/my-first-post.md。打开posts/my-first-post.md，你应该会看到下面的内容：\n+++\ntitle = 'My First Post'\ndate = 2024-01-14T07:07:07+01:00\ndraft = true\n+++\n将title改成你想要的标题，并将draft改为 false。接下来你就可以在下面用 Markdown 的语法写你的博文了。写完之后，可以用hugo server命令启动一个 HTTP 服务器来预览你的网站。确认完没问题后，就可以把更改提交到远程仓库上了。\ngit add .\ngit commit -m \"init blog\"\ngit push\nPush 之后，GitHub 的 Hugo workflow 会自动生成并部署网站。过一段时间后，你应该就可以在<username>.github.io 上看到你的网站了。\n","tags":[]},{"title":"非日本居民能够使用的日本服务","date":"2024-06-08","url":"/posts/japan-services","content":"本文列举了一些非日本居民也能够使用的日本服务（网购、预约系统等等）。非日本居民使用日本服务最大的障碍主要是手机号码。有些日本服务需要日本手机号码的短信验证才能够使用，而非日本居民是无法获得日本手机号码的。\n网购\n駿河屋：注册账号的时候需要填写日本电话号码和住址，不过乱填也可以。支付时可以用 PayPal 刷信用卡。\n卡拉 OK\n日本的卡拉 OK 不需要注册会员就可以用，但是会员往往会有优惠价，所以这里列举的是可以注册会员的店铺。\nジャンカラ：无需任何信息即可注册账号。\nカラオケ館：注册的时候不需要填写手机号码。\nネットカフェ\n快活 CLUB：第一次使用的时候需要到店内用护照注册账号。\n在线订票\nsmartEX：这个网站有分日本版和海外版。日本版必须要日本手机号才能注册，而海外版不用。\nえきねっと：这个网站也有分日本版和海外版，不过日本版的不需要日本手机号也可以使用，因此还是推荐使用日本版。\nハイウェイバスドットコム：不需要日本手机号就能注册。\n高速バスネット：不需要日本手机号就能注册。\n","tags":[]},{"title":"关于LaTeX的各种算法伪代码宏包","date":"2024-05-19","url":"/posts/latex-algorithm-packages","content":"在用 LaTeX 描述算法的时候，经常会用到 algorithm、algorithmic、algorithmicx、algpseudocode、algcompatible、algorithm2e 等各种宏包。这些宏包关系比较复杂，并且用法也存在差异，经常会引发混淆。本文就对这些宏包的作用进行简要的介绍。\n首先，这些宏包可以分为两大类，algorithm2e 单独算作一类，而其他的是另一类。任何算法都可以分为算法标题和算法主体两个部分。对于 algorithm2e 宏包而言，算法标题和算法主体都通过 algorithm 环境来实现。而对于其他的宏包来说，算法标题通过 algorithm 环境实现，算法主体通过 algorithmic 环境实现。也就是说，如果你用的是 algorithm2e 宏包，那么你的算法代码大概就是下面这样：\n\\begin{algorithm}\n\\caption{An algorithm with caption}\\label{alg:cap}\n% 算法主体\n\\end{algorithm}\n而如果用其他的宏包，那么就会变成下面这样：\n\\begin{algorithm}\n\\caption{An algorithm with caption}\\label{alg:cap}\n\\begin{algorithmic}\n% 算法主体\n\\end{algorithmic}\n\\end{algorithm}\n所以说，如果你用的是 algorithm2e 宏包，那么只需要导入这一个宏包就行。algorithm2e 宏包里面包含了一个一步到位的超强 algorithm 环境。而如果你用的是其他宏包，就必须组合使用两个宏包，一个是 algorithm 宏包，它提供了一个专门用于显示算法标题的 algorithm 环境，另外一个则是 algorithmic、algorithmicx、algpseudocode 和 algcompatible 中的其中一个，它们都提供了用于显示算法主体的 algorithmic 环境。\n接下来介绍一下 algorithmic、algorithmicx、algpseudocode 和 algcompatible 的区别。algorithmic 宏包是最早的，现在已经不推荐使用。algorithmicx 是 algorithmic 的改进版，和 algorithmic 不同，algorithmicx 没有提供预定义的语句，只提供了定义语句的命令，因此单纯用 algorithmicx 写伪代码会很麻烦。algpseudocode 和 algcompatible 则是在 algorithmicx 的基础上提供了一组定义好的语句。algpseudocode 和 algcompatible 的主要区别是 algcompatible 额外提供了对于 algorithmic 的兼容性，如果不需要兼容的话，还是更推荐使用 algpseudocode。导入 algpseudocode 或者 algcompatible 都会顺便把 algorithmicx 也导入进来，所以导入了 algpseudocode 或者 algcompatible 就不用再导入 algorithmicx 了。\n最后是省流版：目前 LaTeX 的算法伪代码宏包主要就两种选择。第一种是\\usepackage{algorithm, algpseudocode}，第二种是\\usepackage{algorithm2e}。它们的语法是不一样的，选择一个自己喜欢的就行。\n","tags":[]},{"title":"杠杆ETF中隐藏的陷阱","date":"2025-02-24","url":"/posts/leveraged-etf-trap","content":"很多人会用杠杆ETF来投资，这样可以用更少的本金博取更多的收益，但是，杠杆ETF存在一个很大的陷阱，那就是它的高损耗率。这里的损耗率不是来自于基金的管理费或者折价/溢价率，而是来自于杠杆ETF的内在机制。\n一般的杠杆ETF逻辑是这样的：假设是一个2倍杠杆的ETF，那么这个ETF的净值会保证以下的性质：假设某天其标的资产上涨了1%，那么这个ETF的净值会上涨2%；反之，如果标的资产下跌了1%，那么这个ETF的净值会下跌2%。乍一看，这似乎毫无问题。花10000元投资无杠杆的ETF和花5000元投资2倍杠杆的ETF，应该是等价的。但是，仔细计算一下就会发现问题。\n一个例子\n假设某天标的资产上涨了20%（这里举一个比较大的数字方便比较），第二天又下跌了16.67%，那么这两天标的资产的价格应该是没有变化的。接下来让我们计算一下2倍ETF净值的变化：首先上涨40%，然后下跌33.33%，这样实际上是下跌了6.67%！也就是说，只要标的资产的价格存在震荡，那么杠杆ETF就会发生损耗。\n数学证明\n我们可以用数学推导来更严谨地证明。假设(1+x)(1−y)=1(1+x)(1-y) = 1(1+x)(1−y)=1，那么(1+2x)(1−2y)<1(1+2x)(1-2y)<1(1+2x)(1−2y)<1。这是因为：\n\n(1+x)(1-y) = 1 + x - y - xy = 1 \\Rightarrow x - y = xy\n\n因此\n\n(1+2x)(1-2y) = 1 + 2(x-y) - 4xy = 1 - 2xy < 1\n\n反向杠杆\n那么，反向杠杆是否也会存在类似的损耗呢？答案是肯定的，哪怕是反向1倍杠杆的ETF也会存在损耗。感兴趣的读者可以模仿上面的过程自己推导一下。\n","tags":["理财"]},{"title":"用Makefile编写生成多个目标文件的规则","date":"2024-04-27","url":"/posts/makefile-multiple-outputs","content":"假设现在有一个命令 cmd，它会读取文件 a，然后生成文件 x 和 y。\n#include <stdio.h>\n#include <stdlib.h>\n\nint main() {\n    puts(\"Reading a.\");\n    system(\"cat a\");\n    puts(\"Creating x and y.\");\n    system(\"touch x y\");\n    return 0;\n}\n我们可能很自然地会写出这样的 Makefile 规则：\nall: x y ;\n\nx y: a\n    cmd\n乍一看，这似乎没问题，但是当我们并发运行 make 的时候，问题就出现了。假设我们运行make -j2，就会看到以下输出：\nReading a.\nReading a.\nCreating x and y.\nCreating x and y.\n也就是说，这个命令被执行了两次！这是因为，当我们编写\nx y: a\n    cmd\n的时候，实际上等价于\nx: a\n    cmd\n\ny: a\n    cmd\n这样一来，当多线程运行的时候，make 就会并行地同时生成 x 和 y，导致重复执行。\n后来，我在这个 StackOverflow 问题中找到了解决办法。\nGNU Make v4.3 之后的版本\nGNU Make v4.3 新增了 grouped explicit targets 的特性，我们只需要在冒号前面加一个&就可以解决问题。\nx y &: a\n    cmd\nGNU Make v4.3 之前的版本\n如果需要考虑到版本兼容性，那么还有一种更通用但是更复杂的解决方法，这种方法利用了一个虚拟的中间文件 xy.intermediate。\nx y: xy.intermediate ;\n\n.INTERMEDIATE: xy.intermediate\nxy.intermediate: a\n\tcmd\n为了让这个方法用起来更加方便，我编写了一个 VSCode snippet：\n\"Multiple outputs\": {\n    \"prefix\": [\"multiout\"],\n    \"body\": [\"${1:<TARGETS>}: ${1/[^0-9a-zA-Z_\\\\-\\\\.]//g}.intermediate ;\",\n                \"\",\n                \".INTERMEDIATE: ${1/[^0-9a-zA-Z_\\\\-\\\\.]//g}.intermediate\",\n                \"${1/[^0-9a-zA-Z_\\\\-\\\\.]//g}.intermediate: ${2:<DEPS>}\",\n                \"\\t\"],\n    \"description\": \"A pattern for recipes with multiple outputs.\"\n}\n","tags":[]},{"title":"管理Windows“添加或删除程序”中的项目","date":"2024-05-15","url":"/posts/manage-add-remove-programs","content":"在 Windows 的控制面板中点击“添加或删除程序”，便会看到当前电脑中安装的程序。这些程序的信息实际上是存储在注册表中的HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall。这个注册表项下的每一个子项都代表了一个安装的程序。每个项记录的信息可以参见微软的官方文档。下面简要介绍关键的信息。下面提到的值类型都是字符串。\n\nDisplayName: 程序的名称。\nDisplayIcon: 程序的图标。指定一个 exe 文件的路径，就会以这个 exe 文件的图标作为程序的图标。\nDisplayVersion: 程序的版本。\nPublisher: 程序的发行商。\nInstallDate: 程序的安装日期。格式是 YYYYMMDD，例如 20081225。\nUninstallString: 程序的卸载方式。在这里指定一个 exe 文件的路径，则卸载的时候会执行这个 exe 文件。\n\n","tags":[]},{"title":"一个最简单的Python包示例","date":"2025-03-11","url":"/posts/minimal-python-package","content":"我写了一个最简单的Python包示例，并详细介绍了各个文件的作用。此外还介绍了如何在开发环境下本地安装包、如何构建包以及如何将包发布到PyPI。\n相关内容已经上传到GitHub：shi0rik0/minimal-python-pkg。如果觉得有帮助的话，欢迎给个Star😀。\n","tags":[]},{"title":"现代HTML的最简模板","date":"2024-09-17","url":"/posts/modern-html-template","content":"下面是使用 Vite 创建的前端项目的默认index.html文件内容：\n<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\" />\n    <link rel=\"icon\" href=\"/favicon.ico\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n    <title>Vite App</title>\n  </head>\n\n  <body></body>\n</html>\n下面对这里面的一些关键点进行解说。\n<!DOCTYPE html>\n这一行表示该文件采用的是 HTML 5 标准。\n<html lang=\"en\"></html>\nlang=\"en\"表示这个网页是英文网页，如果是大陆的中文网页，可以写lang=\"zh-CN\"。\n<meta charset=\"UTF-8\" />\n这一行表示源文件使用的编码是 UTF-8。\n<link rel=\"icon\" href=\"/favicon.ico\" />\n这一行指定了网页的图标。\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n这一行是对网页的 viewport 进行设置，使得网页在移动设备上浏览时不会缩放。由于早期大部分网页没有针对移动设备的小屏幕进行适配，因此移动设备访问网页时，默认会渲染一个大于屏幕尺寸的网页，然后用户可以通过手指触摸操作来移动网页的显示区域。由于现在很多网页都是原生适配移动端的，在这种情况下缩放就没有必要了，可以通过这个设置来禁用缩放。\n<title>Vite App</title>\n这一行指定了网页的标题。\n","tags":[]},{"title":"Nginx入门教程","date":"2024-09-05","url":"/posts/nginx-tutorial","content":"启动、终止与重新加载\n启动 nginx：\nnginx -c conf/nginx.conf\n-c参数可以指定配置文件的路径，默认为conf/nginx.conf。\n终止 nginx：\nnginx -s quit\n重新加载配置文件：\nnginx -s reload\n最简单的配置文件\n下面的配置文件会监听 8000 端口，并将网站根目录设为/www/data。\nevents {\n\n}\n\nhttp {\n    server {\n        listen 8000;\n\n        location / {\n            root /www/data;\n        }\n    }\n}\n这里要补充几点，首先是 http 块下面可以有多个 server 块，例如：\nhttp {\n    server {\n        listen 8000;\n\n        location / {\n            root /www/data;\n        }\n    }\n\n    server {\n        listen 8001;\n\n        location / {\n            root /www/data2;\n        }\n    }\n}\n这样就会同时监听 8000 和 8001 两个端口。\n还有一点就是location /中的/是用来匹配网站的根目录，这里也就是匹配了网站的所有路径。一个 server 块下面也可以有多个 location 块，用来匹配不同的路径。比如说：\nhttp {\n    server {\n        listen 8000;\n\n        location / {\n            root /www/data;\n        }\n\n        location /images/ {\n            root /www/data2;\n        }\n    }\n}\n这样一来，访问http://localhost:8000/images/img.png的时候就会返回/www/data2/images/img.png。\n反向代理\nhttp {\n    server {\n        listen 8000;\n\n        location / {\n            proxy_pass http://127.0.0.1:8001;\n            proxy_set_header Host $host;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        }\n    }\n}\n上面就是一个最基本的反向代理配置，会将 8000 端口收到的消息转发到本地的 8001 端口。两行proxy_set_header配置是可选的，它们会在转发消息的头部增加Host和X-Forwarded-For两个字段，这些字段可能会被一些上游应用使用。\n负载均衡\nhttp {\n    upstream backend {\n        server 127.0.0.1:8001 weight=3;\n        server 127.0.0.1:8002;\n    }\n\n    server {\n        listen 8000;\n\n        location / {\n            proxy_pass http://backend;\n            proxy_set_header Host $host;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        }\n    }\n}\n上面就是一个最基本的负载均衡配置。Nginx 默认采用的是 Round Robin 算法，每个服务器默认权重为 1。上面的配置文件下，两个上游服务器的负载比约为 3：1。\n","tags":[]},{"title":"关于npm install的开发依赖","date":"2024-11-25","url":"/posts/npm-install-save-dev","content":"npm在管理依赖的时候，会将依赖分为生产依赖和开发依赖。生产依赖就是运行的时候用到的依赖，而开发依赖则是测试框架、linter 等开发时才会用到的辅助工具。\n如果你用npm install {package}，那么默认就是作为生产依赖去安装。要安装一个开发依赖，就要指定--save-dev或者-D：\nnpm install --save-dev {package}\n# 或者\nnpm install -D {package}\n（有的教程会说，安装生产依赖的时候要指定--save，也就是：npm install --save {package}，这是过时的说法，新版本npm已经将此作为默认值了。）\n在部署的时候，如果不希望安装开发依赖，那么可以用--omit=dev参数：\nnpm install --omit=dev\n（有的教程会说用--production参数，但是新版本npm已经不推荐使用这个了，如果用这个，会得到一个 warning。）\n（吐槽时间：npm怎么老是随意更改接口。）\n","tags":[]},{"title":"如何将包发布到npm","date":"2025-03-05","url":"/posts/npm-upload","content":"首先要在npmjs.com上注册一个账号。\n在发布包之前，要检查package.json中的name字段是否正确，这个字段就决定了在npm上的包名。\n如果没问题，就可以发布了，首先要登录npm账号：\nnpm login\n然后发布：\nnpm publish --access public\n就是这么简单！\n","tags":["npm"]},{"title":"Pinia快速入门","date":"2025-01-12","url":"/posts/pinia-tutorial","content":"Pinia 是 Vue 3 官方的状态管理库，通俗点说就是可以给 Vue 应用程序提供一个“全局变量”。Pinia 的基本使用方法非常简单。\n导入 Pinia\n首先，我们需要安装 Pinia：\nnpm install pinia\n然后在 Vue 3 项目中导入 Pinia：\nimport { createApp } from 'vue'\nimport { createPinia } from 'pinia' // <--\nimport App from './App.vue'\n\nconst pinia = createPinia() // <--\nconst app = createApp(App)\n\napp.use(pinia) // <--\napp.mount('#app')\n创建一个 Store\nStore 是 Pinia 的一个核心概念，简单来说就是一组全局变量集合。我们可以通过 defineStore 函数来创建一个 Store：\nimport { defineStore } from 'pinia'\n\nexport const useExampleStore = defineStore('example', {\n  state: () => ({\n    count: 0,\n  }),\n  getters: {\n    doubleCount: (state) => state.count * 2,\n  },\n  actions: {\n    increment() {\n      this.count++\n    },\n  },\n})\ndefineStore函数接受两个参数，第一个参数是 Store 的全局 ID，第二个参数是 Store 的配置。配置中包含了 state、getters 和 actions 三个属性，分别用来定义 Store 的状态、计算属性和方法。\n使用 Store\n在 Vue 组件中使用 Store 非常简单，只要 import 并调用 Store 的 use 函数即可：\n<script setup>\nimport { useExampleStore } from '../stores/example'\n\nconst store = useExampleStore()\n</script>\n\n<template>\n  <div>\n    <p>Count: {{ store.count }}</p>\n    <p>Double Count: {{ store.doubleCount }}</p>\n    <button @click=\"store.increment\">Increment</button>\n  </div>\n</template>\n要注意，Store 对象本身就是响应式的，所以读取或修改状态的时候不需要使用.value:\n<script setup>\nimport { useExampleStore } from '../stores/example'\n\nconst store = useExampleStore()\n\nconst increment = () => {\n  // 错误的写法：store.count.value++\n  store.count++\n}\n</script>\n\n<template>\n  <div>\n    <p>Count: {{ store.count }}</p>\n    <button @click=\"increment()\">Increment</button>\n  </div>\n</template>\n深入 state\n在定义state的时候，Pinia 会自动推断类型，但是有时候我们需要手动指定类型：\ndefineStore('example', {\n  state: () => ({\n    names: [] as string[],\n  }),\n})\n深入 getters\n在 getter 中访问 state 的时候利用 state 参数（第一个参数）就可以了。如果希望在 getter 中使用其他 getter，可以使用this关键字，但是这时候就不能使用箭头函数了，并且这时候还需要手动指定返回值类型：\ndefineStore('example', {\n  getters: {\n    doubleCount: (state) => state.count * 2,\n    quadrupleCount(state): number {\n      return this.doubleCount * 2\n    },\n  },\n})\n深入 actions\n一个 action 可以是异步函数。\n在 action 中，可以直接访问其他 state、getter 和 action，并且都是通过this来访问，因此 action 也不能是箭头函数。\n","tags":["pinia"]},{"title":"解决Pixel 8无法使用中国SIM卡接打电话的问题","date":"2024-07-19","url":"/posts/pixel-volte-in-china","content":"最近购买了一台日版的 Pixel 8，发现使用国内的 SIM 卡的时候可以流量上网，但无法正常接打电话和收发短信。经过调查，发现问题的根源是这样的：\n\n由于 Pixel 手机没有在国内上市，所以谷歌在 Pixel 上加了地区锁，导致国内 SIM 卡无法使用 5G，最多只能用到 4G。\n国内的 4G 通信基本上都使用了一种名为 VoLTE 的技术。这种技术需要手机的配置文件里有运营商的信息。由于 Pixel 手机没有在国内上市，自然 Pixel 里面也没有写入国内运营商的信息，因此无法使用 VoLTE。\n\n综上所述，解决办法有两种：如果是 5G SIM 卡，那么可以尝试 root 之后解开地区锁使用 5G。还有一种更通用的办法是修改配置文件，让 Pixel 能够使用 4G VoLTE。修改配置文件无需 root。\n接下来介绍免 root 开启 VoLTE 的方法。\n首先在 Google Play 上安装 Shizuku 和 Pixel IMS。\n然后打开 Shizuku，点击“配对”，然后点击“开发者选项”，然后打开“无线调试”的开关，然后进入“无线调试”的菜单（点击“无线调试”的开关按钮的左边），然后点击“使用配对码配对设备”，这时候会看到一个 6 位数的 WLAN 配对码。然后通知栏里 Shizuku 会提示你输入配对码，输入后回到 Shizuku，点击“启动”即可。\n然后打开 Pxiel IMS。允许 Shizuku 给 Pxiel IMS 授权。然后在 SIM 卡配置的菜单里打开“启用 VoLTE”的选项即可。\n注意：通过这种方法开启的 VoLTE，在手机重启之后就会失效。需要重新按照上面的方法再启用一次。\n参考资料\nhttps://cnzhx.net/blog/enable-volte-for-pixel-6-7-without-root/\n","tags":[]},{"title":"PowerShell脚本对于双横线的特殊处理","date":"2025-01-15","url":"/posts/powershell-double-hyphen","content":"最近在运行npm create vue@latest -- --help的时候，发现一个有趣的现象：如果用 PowerShell 运行这个命令，会输出npm create的帮助信息，而用 cmd 运行就没有问题。后面经过调查，发现是 PowerShell 脚本对于双横线的特殊处理导致的。\nnpm 针对 cmd 和 PowerShell 提供了两个不同的脚本，分别是npm.cmd和npm.ps1。在 PowerShell 中运行npm的时候，实际上会执行npm.ps1。在 PowerShell 脚本中，--有特殊含义，因此执行npm create vue@latest -- --help就近似于执行npm create vue@latest --help，因此会输出npm create的帮助信息。\n解决方法很简单，只需要将--用引号括起来即可，即npm create vue@latest '--' --help。这样 PowerShell 就不会对--进行特殊处理了。\n具体可以参考这个GitHub PR。\n这里还要补充一点，这个问题只会出现在执行 PowerShell脚本的时候，用 PowerShell 执行普通二进制文件是不会有这个问题的。\n","tags":[]},{"title":"PowerShell脚本被禁止执行的解决方法","date":"2024-04-09","url":"/posts/powershell-policy","content":"在 Windows 中执行 PowerShell 脚本的时候，默认情况下会得到下面的错误信息：\n\n无法加载文件 xxx.ps1，因为在此系统上禁止运行脚本。有关详细信息，请参阅 https:/go.microsoft.com/fwlink/?LinkID=135170 中的 about_Execution_Policies。\n\n这是因为默认情况下，Windows 的 PowerShell 脚本执行策略是Restricted，也就是阻止运行任何脚本。要允许执行任意脚本，只需要将执行策略修改为Unrestricted即可：\nSet-ExecutionPolicy -ExecutionPolicy Unrestricted -Scope CurrentUser\n","tags":[]},{"title":"《基于启动和脑电波实验研究普通话和闽南语连续变调词的储存模式》书摘","date":"2024-05-15","url":"/posts/priming-and-erp","content":"最近粗略翻阅了一本学术著作《基于启动和脑电波实验研究普通话和闽南语连续变调词的储存模式》。书中第二章和第三章针对普通话和闽南话的变调规则做了一个实验。实验流程是这样的，以普通话的“3-3→2-3”规则（比如辅导 fu3dao3 实际上会念成 fu2dao3）为例。实验人员让被试者听一些词语，然后让被试者判断这些词语是不是真实存在的词语。在让被试者听一个词语之前，会播放这个词的第一字作为刺激。实验分为两组，第一组播放的刺激是变调前的发音，比如播放“辅导”之前会先播放“fu3”的声音。另一组则会提前播放变调后的发音，比如播放“辅导”之前会先播放“fu2”的声音。然后比较两组实验中被试者的反应时间和准确率。除了普通话的“3-3→2-3”规则，研究者还针对闽南语的“24→33”规则（例如“平等”）和“51→55”规则（例如“主持”）也做了相同的实验。\n实验结果如下表所示：\n\n\n\n变调规则\n刺激效果（变调前）\n刺激效果（变调后）\n\n\n\n\n普通话“3-3→2-3”\n有\n无\n\n\n闽南语“24→33”\n有（较好）\n有（较差）\n\n\n闽南语“51→55”\n有（较差）\n有（较好）\n\n\n\n表格中所说的“有刺激效果”的意思是，播放刺激声音后，被试者的反应速度和准确率提高了。\n从实验结果可以看出，对于不同变调规则，播放变调前和变调后的声音的刺激效果是不一样，并且两者之间存在负相关的关系。\n作者还认为，播放变调前声音的刺激效果与变调规则的能产性（productivity）是正相关的。依据是有文献（Modeling Taiwanese speakers’ knowledge of tone sandhi in reduplication）表明闽南语“24→33”规则的能产性高于“51→55”规则。\n在该文献中，研究者做了这么一个实验：他们给闽南语母语者播放一个字 A，然后让被试者念出这个字的重叠形式 AA，例如“怪”→“怪怪”，然后分析变调的准确率。用于测试的字可以分为三组：AO、*AO 和 AG。其中 AG 是并不存在的字（不算声调的话，发音是合法的，但是算上声调就不存在这么发音的字），而 AO 和*AO 都是真实存在的字，不过对于 AO 组的字而言，其重叠形式的词是真实存在的，而*AO 组中的字并不存在重叠形式的词。下图给出了这三组字的例子：\n\n该实验的测试结果如下图所示：\n\n从中可以明显看出“24→33”规则的变调准确率远远高于“51→55”规则，特别是对于 AG 组。这就说明“24→33”规则的能产性更高。\n","tags":[]},{"title":"代理服务器设置方法","date":"2025-01-06","url":"/posts/proxy-config","content":"由于 GFW 的存在，在中国境内访问境外的网站有时候会非常的慢，这时候就只能使用代理，但是有些程序没有使用系统全局的代理设置，需要自己手动配置代理。本文总结了一些需要手动配置的程序的配置方法。\ngit\nGit 没有专门针对 HTTPS 协议的代理设置，不管是 HTTP 还是 HTTPS，都是使用 http.proxy 来设置代理。\ngit config --global http.proxy # 查看代理设置\ngit config --global http.proxy http://example.com:8000\ngit config --global --unset http.proxy\nnpm\nnpm config get proxy\nnpm config get https-proxy\nnpm config set proxy http://example.com:8000\nnpm config set https-proxy http://example.com:8000\nnpm config delete proxy\nnpm config delete https-proxy\n","tags":[]},{"title":"在pybind11中操作numpy数组","date":"2024-12-29","url":"/posts/pybind11-numpy","content":"最近需要给 Python 写一个涉及 numpy 数组操作的 C++扩展。我用的是pybind11，用这个库写 Python C++拓展非常方便，并且这个库也提供了对于 numpy API 的封装，可惜官方文档写得比较晦涩，并且也不太全面。我结合官方文档和源代码，总结了一下 numpy 数组的操作方法。\n头文件\n首先要 include 以下头文件：\n#include <pybind11/buffer_info.h>\n#include <pybind11/numpy.h>\n#include <pybind11/pybind11.h>\n\nnamespace py = pybind11;\nnumpy 数组在 pybind11 中的类型\n在pybind11中，numpy 数组的类型是py::array。如果要限定数组的dtype，还可以用py::array_t<T>，其中T是数组元素的类型。例如，如果数组元素是double类型，那么数组的类型就是py::array_t<double>。\n获取 numpy 数组的信息\n获取 numpy 数组的信息可以使用py::array_t对象的request()方法，这个方法返回一个py::buffer_info对象，这个对象包含了数组的维度、元素类型、元素大小等信息。例如：\nvoid print_array_info(py::array arr) {\n    py::buffer_info info = arr.request();\n    std::cout << \"ndim: \" << info.ndim << std::endl;\n    std::cout << \"shape: \";\n    for (auto s : info.shape) {\n        std::cout << s << \" \";\n    }\n    std::cout << std::endl;\n    std::cout << \"dtype: \" << info.format << std::endl;\n    std::cout << \"itemsize: \" << info.itemsize << std::endl;\n}\n假如我们这么调用这个函数：\narr = np.zeros((3, 4), dtype=np.int32)\nm.print_array_info(arr)\n那么输出结果就是：\nndim: 2\nshape: 3 4\ndtype: l\nitemsize: 4\n访问、修改数组元素\n如果只是访问数组元素，可以使用unchecked()方法，这个方法会返回一个 proxy 对象，通过这个对象可以直接访问数组元素。例如：\nvoid print_2d_array(py::array_t<double> arr) {\n    if (arr.request().ndim != 2) {\n        throw std::runtime_error(\"only 2D array is supported\");\n    }\n    auto shape = arr.request().shape;\n    auto proxy = arr.unchecked();\n    for (int i = 0; i < shape[0]; i++) {\n        for (int j = 0; j < shape[1]; j++) {\n            std::cout << proxy(i, j) << \" \";\n        }\n        std::cout << std::endl;\n    }\n}\n如果还要修改元素，就需要使用mutable_unchecked()。例如：\nvoid add_one(py::array_t<double> arr) {\n    if (arr.request().ndim != 1) {\n        throw std::runtime_error(\"only 1D array is supported\");\n    }\n    auto size = arr.request().shape[0];\n    auto proxy = arr.mutable_unchecked();\n    for (int i = 0; i < size; i++) {\n        proxy(i) += 1;\n    }\n}\n这里还要注意，py::array_t是一个引用类型，在传参过程中底层数据不会被复制，所以在函数内部修改数组元素会影响到原数组。\nunchecked()和mutable_unchecked()方法都接受一个可选的模板参数，代表数组的维度。指定维度可以让编译器生成更高效的代码。例如：\nauto proxy = arr.unchecked<2>();\n创建、返回 numpy 数组\n如果要创建 numpy 数组，使用py::array_t的构造函数即可。其中一个构造函数接受数组的维度，然后返回指定维度的数组。要注意的是，返回值不会被初始化为全 0。这个构造函数的参数相当泛型，可以接受std::vector、std::initializer_list等类型。例如：\npy::array_t<double> create_array(int size) {\n    return py::array_t<double>({size});\n    // 因为是一维数组，所以也可以用 py::array_t<double>(size)\n}\n这个例子也展示了如何返回 numpy 数组，直接 return 即可。因为py::array_t是一个引用类型，所以返回的过程不会发生复制。\n","tags":[]},{"title":"关于Pydantic model字段的别名","date":"2024-06-22","url":"/posts/pydantic-field-alias","content":"给 model 字段指定别名\nPydantic 的 model 字段名是通过 Python 的类型注解设定的。这样一来当 schema 的字段名不是合法的 Python 变量名的时候就会带来问题。这种情况下可以用 Field 对象的 alias 属性来给字段指定名称。例如下面的例子给family_name这个字段设定了别名family-name。\nfrom pydantic import BaseModel, Field\n\n\nclass Person(BaseModel):\n    family_name: str = Field(alias='family-name')\n\n\njson_str = '{\"family-name\": \"Smith\"}'\nperson = Person.model_validate_json(json_str)\nprint(person.model_dump_json())\n上面的代码输出为：\n{\"family_name\":\"Smith\"}\n可见，默认设置下 Pydantic 输出时会使用本名而非别名。如果希望使用别名，可以设置参数by_alias=True，也就是把代码最后一行改成：\nprint(person.model_dump_json(by_alias=True))\n使用 alias generator 自动生成别名\n当一个 model 包含很多字段的时候，逐个手动填写别名就会很麻烦。这时候可以用 alias generator 自动生成别名。一个最简单的 alias generator 就是一个 str->str 的函数。通过设置 model 的 alias generator 参数就可以自动生成别名：\nfrom pydantic import BaseModel, ConfigDict\n\n\ndef my_alias_generator(name: str) -> str:\n    return name.replace('_', '-')\n\n\nclass Person(BaseModel):\n    model_config = ConfigDict(alias_generator=my_alias_generator)\n    family_name: str\n    given_name: str\n\n\njson_str = '{\"family-name\": \"Smith\", \"given-name\": \"Alice\"}'\nperson = Person.model_validate_json(json_str)\nprint(person.model_dump_json(by_alias=True))\npydantic.alias_generators模块中已经提供了一些常见的 alias generator，可以直接使用。\n","tags":[]},{"title":"使用Docker搭建PySpark学习环境","date":"2024-06-20","url":"/posts/pyspark-docker-env","content":"首先安装 Docker，然后拉取 Spark 的 Docker 镜像：\ndocker pull apache/spark\n然后创建一个 Docker 容器：\ndocker run -d -u root apache/spark tail -f /dev/null\n（注：这里的-u root 的作用是把容器的默认用户设置成 root，如果不这么做，VSCode 就无法附加到容器上。）\n接着就可以使用 VSCode 进入容器。进入后在终端里打开 spark-shell：\n/opt/spark/bin/spark-shell\n打开之后会看到类似下面的输出，里面包含了 Spark 的版本。这里我的版本是 3.5.1。\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.1\n      /_/\n然后安装对应版本的 PySpark：\npip install pyspark==3.5.1\n注意：这里的 PySpark 版本一定要和 Spark 版本一致。如果不一致，并不会立刻报错，而是会出现有些代码可以运行，有些代码不能运行的情况！\n安装完 PySpark 之后，就可以用 Python 编写 Spark 程序了。下面是一个简单的例子：\nfrom pyspark import SparkContext\n\nsc = SparkContext.getOrCreate()\n\nrdd = sc.parallelize(['apple', 'apple', 'banana', 'banana', 'banana']).map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\nprint(rdd.collect())\n你应该会看到类似下面的输出：\n[('banana', 3), ('apple', 2)]\n","tags":[]},{"title":"Python 3.12的一个破坏兼容性的更改：移除distutils","date":"2024-06-10","url":"/posts/python-3-12-distutils","content":"最近使用 Python 3.12 import keras 的时候，遇到了如下的错误：\nModuleNotFoundError: No module named 'distutils'\n根据StackOverflow 的热心网友的解释，这是因为 Python 3.12 不再预装 distutils 了，可以通过手动安装 setuptools 来安装 distutils。但是我安装了 setuptools 后依然不能解决问题，因此最终被迫降级到 3.11。\n在我看来，这种对兼容性的破坏是没有必要的。目前我还是推荐使用 Python 3.11。\n","tags":[]},{"title":"在Python中更优雅地创建抽象类及接口","date":"2025-01-05","url":"/posts/python-abstract-class","content":"以前我创建抽象类或者接口的时候，通常会这样写：\nclass AbstractClass:\n    def method(self, arg: int) -> int:\n        raise NotImplementedError()\n这么做会带来以下问题：\n\n如果子类没有实现抽象方法，那么在调用该方法的时候才会报错，而不是在创建对象的时候就报错。\n无法强制子类按照 type hints 的要求来实现方法。\n\n下面就介绍一下如何解决这两个问题。\n强制子类实现抽象方法\nPython 标准库提供了一个类abc.ABC，如果一个类继承了abc.ABC，那么这个类就是一个抽象类。我们可以使用@abc.abstractmethod装饰器来标记一个方法是抽象方法。抽象类的子类必须实现所有抽象方法，否则在创建对象的时候就会报错。\nfrom abc import ABC, abstractmethod\n\nclass AbstractClass(ABC):\n    @abstractmethod\n    def method(self, arg: int) -> int:\n        ...\n\nclass ConcreteClass(AbstractClass):\n    def method(self, arg: int) -> int:\n        return arg\n\nc = ConcreteClass() # 没问题\na = AbstractClass() # 会得到 TypeError\n确保子类正确 override 方法\n从 Python 3.12 开始，typing模块新增了一个@override装饰器。这个装饰器可以提醒静态检查器这个方法必须覆盖父类的方法，并且方法签名必须一致。\nfrom typing import override\n\nclass AbstractClass:\n    def method(self, arg: int) -> int:\n        raise NotImplementedError()\n\nclass ConcreteClass(AbstractClass):\n    @override\n    def method(self, arg: int) -> None: # 会引发静态检查器的警告\n        pass\n如果是 Python 3.11 及以下的版本，可以用typing_extensions模块提供的 backport。\n","tags":[]},{"title":"Python标记deprecated的装饰器","date":"2025-03-19","url":"/posts/python-deprecated-decorator","content":"从Python 3.13开始，warnings模块里新增了一个@deprecated装饰器，用于标记函数等已经被启用。具体可以参考Python文档和PEP 702。\n下面是一个简单的例子：\nfrom warnings import deprecated\n\n@deprecated(\"use new_func instead\")\ndef old_func():\n    pass\nVS Code已经提供了对于@deprecated装饰器的支持，如果调用了被弃用的函数，会在函数上看到一个删除线，将鼠标悬停在上面可以看到警告信息。\n\n如果你还在使用Python 3.12或者更早的版本，可以使用typing_extensions模块里的@deprecated装饰器来代替，这是标准库的一个移植。（不过这里有点奇怪：typing_extensions应该是typing模块的移植，怎么把warnings模块也移植过来了？）\n","tags":[]},{"title":"用Python实现经典设计模式（更新中）","date":"2024-12-06","url":"/posts/python-design-patterns","content":"观察者模式（Observer Pattern）\n所谓观察者模式，就是一个简单的消息通知机制。当一个对象想要发送消息时，订阅了这个消息的对象会收到通知。\n假设我们有一个杂志社MagazinePublisher，它有很多订阅者Subscriber，当杂志社有新的杂志发布时，会通知所有的订阅者。\nclass Subscriber:\n    def __init__(self, name):\n        self.name = name\n\n    def update(self, message):\n        print(f'{self.name} received message: {message}')\n\nclass MagazinePublisher:\n    def __init__(self):\n        self.subscribers = []\n\n    def add_subscriber(self, subscriber):\n        self.subscribers.append(subscriber)\n\n    def remove_subscriber(self, subscriber):\n        self.subscribers.remove(subscriber)\n\n    def notify_subscribers(self, message):\n        for subscriber in self.subscribers:\n            subscriber.update(message)\n\nsubscriber1 = Subscriber('Alice')\nsubscriber2 = Subscriber('Bob')\npublisher = MagazinePublisher()\npublisher.add_subscriber(subscriber1)\npublisher.add_subscriber(subscriber2)\npublisher.notify_subscribers('New magazine is published!')\n上面代码执行后的输出是：\nAlice received message: New magazine is published!\nBob received message: New magazine is published!\n简单工厂模式（Simple Factory Pattern）\n所谓简单工厂模式，就是一个函数的返回类型会因为传入的参数不同而有所不同。\n假设我们有一个汉堡店，可以根据顾客需求返回不同的汉堡。\nclass Hamburger:\n    def eat(self):\n        print(f'Eating a {self._type} hamburger')\n\nclass BeefHamburger(Hamburger):\n    def __init__(self):\n        self._type = 'beef'\n\nclass ChickenHamburger(Hamburger):\n    def __init__(self):\n        self._type = 'chicken'\n\ndef make_hamburger(hamburger_type):\n    if hamburger_type == 'beef':\n        return BeefHamburger()\n    elif hamburger_type == 'chicken':\n        return ChickenHamburger()\n    else:\n        raise ValueError('Invalid hamburger type')\n\nbeef_hamburger = make_hamburger('beef')\nbeef_hamburger.eat()\nchicken_hamburger = make_hamburger('chicken')\nchicken_hamburger.eat()\n上面代码执行后的输出是：\nEating a beef hamburger\nEating a chicken hamburger\n单例模式（Singleton Pattern）\n所谓单例模式，就是一个类只能有一个实例。在 Python 中，最简单的实现方式就是利用 module，因为一个 Python module 只会执行一次。\n# singleton.py\nclass Singleton:\n    def __init__(self):\n        if hasattr(self.__class__, '_created'):\n            raise ValueError('Singleton instance already exists')\n        self.__class__._created = True\n\n    def do_something(self):\n        print('Doing something')\n\nsingleton = Singleton()\n# main.py\nfrom singleton import singleton\n\nsingleton.do_something()\n策略模式（Strategy Pattern）\n策略模式本质上就是利用了组合的思想，把类的一些功能“外包”给其他类。\n（还在更新中……）\n","tags":[]},{"title":"Python的代码格式化程序对比","date":"2024-06-19","url":"/posts/python-formatters","content":"知名的 Python 代码格式化程序主要有 3 个：autopep8、black和yapf。其中 autopep8 是最古老的，它的开发已经不是很活跃了，因此不推荐使用。black 和 yapf 的区别主要在于 yapf 提供了更多的格式化选项，让程序员可以根据自己的喜好调节代码风格。而 black 的作者有比较严重的洁癖，主张所有的 Python 代码都应该是统一的风格，因此几乎没有提供自定义选项。black 甚至会强制要求字符串用双引号包裹。由于本人没有强迫症，因此还是更倾向于使用 yapf。\n在 VSCode 上就有一个现成的名为 yapf 的插件，只要安装这个插件就可以自动使用 yapf 格式化 Python 代码了。\n","tags":[]},{"title":"Python的坑：隐式字符串连接","date":"2025-01-13","url":"/posts/python-implicit-str-concat","content":"最近写 Python 代码的时候遇到了一个坑：Python 和 C 语言一样，支持隐式字符串连接。所谓隐式字符串连接，就是当两个字符串字面量相邻的时候，会自动连接成一个字符串。比如下面这段代码：\ns = 'hello' 'world'\nassert s == 'helloworld'\n利用这个语法可以将长字符串拆分成多行，但是也可能带来问题，比如下面这段代码：\nnames = [\n    'Alice'  # 这里漏了一个逗号\n    'Bob',\n    'Charlie',\n]\n这段代码的本意是创建一个包含 3 个人名的 list，但是由于缺少逗号，实际上创建的是一个只包含 2 个人名的 list。\n好在 Pylint 可以检测这个问题，并给出警告 W1404，但是 Pylint 默认不会警告跨行的字符串连接，所以还需要手动在 Pylint 的配置中加入以下内容：\n[STRING_CONSTANT]\ncheck-str-concat-over-line-jumps = true\n","tags":[]},{"title":"Python内置函数的key参数","date":"2024-06-19","url":"/posts/python-key-parameter","content":"Python 内置的 sort()、sorted()、max()和 min()都有一个可选的 key 参数。通过这个参数可以指定比较的基准。下面提供一些范例。\n下面的代码会将 list 中的点按照 l2-norm 排序：\na = [(3, 4), (1, 6), (2, 5)]\nprint(sorted(a, key=lambda x: x[0] ** 2 + x[1] ** 2))\n下面的代码可以获取 list 中 l2-norm 最大的点：\na = [(3, 4), (1, 6), (2, 5)]\nprint(max(a, key=lambda x: x[0] ** 2 + x[1] ** 2))\n","tags":[]},{"title":"Python循环语句中的else子句","date":"2024-05-16","url":"/posts/python-loop-else-clause","content":"Python 的循环语句（while 和 for 语句）有一个特殊的语法，就是它们支持 else 子句。下面用一个例子来说明其作用。\nfor i in a:\n    if find(i):\n        do_something(i)\n        break\nelse:\n    not_found()\n在这个代码片段中，我们尝试在容器a中寻找第一个符合某种条件的元素，然后对这个元素进行某些处理。如果在容器中没有找到符合条件的元素，就执行另外的逻辑。\n简单来说，当一个循环语句是正常结束的时候（也就是说不是通过 break 中止的），else 子句才会执行。\n如果不用 else 子句，那么就必须引入一个额外的 bool 变量才能实现相同的逻辑：\nfound = False\nfor i in a:\n    if find(i):\n        found = True\n        do_something(i)\n        break\nif not found:\n    not_found()\n最后指出一点，else 子句早在 Python 2 就已经被支持了，这不是一个最近才添加的语法糖，所以这个特性适用于几乎所有 Python 版本。\n","tags":[]},{"title":"理清Python的包管理系统","date":"2025-01-05","url":"/posts/python-packaging","content":"现代的编程语言都会提供官方的包管理系统，例如 Rust 的 Cargo、Node.js 的 npm、Go 的 go mod 等等。Python 也不例外，但是 Python 的包管理系统非常的混乱，这里主要介绍一下 pip 和 setuptools 这两个工具。\npip 和 setuptools 的关系\nsetuptools 是 Python 的构建和打包工具，它负责构建 Python 项目，并将其打包成一个 wheel 文件，而 pip 是 Python 的包管理工具，它可以直接安装 wheel 文件，也可以调用 setuptools 来打包项目并安装。pip 还负责管理包的依赖关系，它会在安装包的时候自动安装依赖。\nsetuptools 并不是 Python 官方支持的唯一打包工具，但它是最流行的，并且它的功能已经足够使用了，所以大部分 Python 项目都使用 setuptools 来打包项目。\nPython 项目的配置文件\n由于历史问题，Python 项目的配置文件非常混乱，主要有 3 个：setup.py、setup.cfg和pyproject.toml。其中setup.py和setup.cfg是 setuptools 的配置文件。它们的区别在于，setup.py就是一个 Python 脚本，setuptools 会执行这个脚本，并根据脚本的变量值来配置项目。后来，setuptools 的维护者们认为这么做一方面不安全（因为脚本可以执行恶意代码），一方面也过于动态也会影响可复现性，所以他们引入了setup.cfg这个静态配置文件，一部分setup.py的配置可以通过setup.cfg来配置。而pyproject.toml是 PEP 518 提出的，它的目标是统一 Python 项目的配置文件，因为 Python 的打包工具不止 setuptools 一个，并且一个 Python 项目除了打包工具之外，还可能有其他需要配置的工具，例如 linters、formatters 等等。pyproject.toml是一个 TOML 格式的配置文件，Python 官方希望所有的工具都使用这个配置文件来配置项目，所以 setuptools 自然也跟进了，现在 setuptools 的配置也可以写在pyproject.toml中。\n这里总结一下，pyproject.toml是一个统一的配置文件，可以配置任何工具，而setup.py和setup.cfg是 setuptools 的配置文件，用来配置 setuptools。\n如果只是配置 setuptools，那么这 3 个都可以，但是setup.py明显是最灵活最强大的，所以我还是喜欢使用setup.py。又因为 Python 官方推荐使用pyproject.toml，所以我会在项目中同时使用setup.py和pyproject.toml，setup.py只负责配置 setuptools，而pyproject.toml主要负责指定项目的打包工具是 setuptools，以及配置其他工具。\n下面的pyproject.toml会告诉 pip 使用 setuptools 来打包项目。一个 Python 项目的pyproject.toml至少要包含这些内容：\n[build-system]\nrequires = [\"setuptools >= 61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n这样一来，通过 pip 从源码安装包的时候，pip 就会调用 setuptools 来打包项目，而 setuptools 会综合setup.py、setup.cfg和pyproject.toml的配置来打包项目。\n","tags":[]},{"title":"在Python中用进程池异步执行任务","date":"2025-03-25","url":"/posts/python-process-pool-async","content":"废话不多说，先直接上代码：\nimport asyncio\nfrom concurrent.futures import ProcessPoolExecutor\nimport multiprocessing\nimport os\n\nvar_per_process = None\n\n\ndef initialize_process():\n    global var_per_process\n    var_per_process = os.getpid()\n\n\ndef add(arg1, arg2):\n    print(f\"Process {var_per_process} received {arg1} and {arg2}\")\n    return arg1 + arg2\n\n\nasync def main():\n    loop = asyncio.get_event_loop()\n    with ProcessPoolExecutor(\n        max_workers=multiprocessing.cpu_count(), initializer=initialize_process\n    ) as executor:\n        inputs = [(1, 2), (3, 4), (5, 6)]\n        results = await asyncio.gather(\n            *[loop.run_in_executor(executor, add, i, j) for i, j in inputs]\n        )\n        print(results)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nProcessPoolExecutor的构造函数有一个initializer参数，可以传入一个函数，这个函数会在每个进程启动的时候被调用。这个函数可以用来初始化每个进程的状态，比如连接数据库等。\nloop.run_in_executor的返回值是一个Future对象。将多个Future对象传入asyncio.gather，就可以异步执行多个任务，并等待所有任务完成。\n执行这段代码，会看到类似下面的输出：\nProcess 4088441 received 1 and 2\nProcess 4088442 received 3 and 4\nProcess 4088443 received 5 and 6\n[3, 7, 11]\n","tags":[]},{"title":"Python性能分析教程","date":"2025-03-20","url":"/posts/python-profile","content":"Python标准库里面有两个可以用于性能分析的模块：cProfile 和 profile。cProfile是用C编写的，而profile是用纯Python实现的。它们的接口类似，但是并不完全相同，比如说它们的输出文件格式不同。有一个知名的库snakeviz可以用来可视化cProfile的输出文件，但是它不支持profile的输出文件。cProfile不仅更快，还被snakeviz所支持，所以大多数时候还是推荐使用cProfile。\ncProfile有很多接口，但是最简单的用法就是使用python -m cProfile命令直接测试一个脚本：\npython -m cProfile -o out_file src.py\n然后就可以用snakeviz来可视化out_file的内容了：\npip install snakeviz\nsnakeviz out_file\n","tags":[]},{"title":"Python标准库的简易HTTP服务器","date":"2024-06-30","url":"/posts/python-stdlib-http-server","content":"Python 标准库有一个名为http.server的模块，提供了简易的 HTTP 服务器功能。下面的示例代码用这个模块启动了一个简单的 HTTP 服务器。这个服务器只支持 POST 方法，会打印 POST 请求的信息，并返回一个固定的字符串。\nfrom http.server import BaseHTTPRequestHandler, HTTPServer\n\n\nclass MyHandler(BaseHTTPRequestHandler):\n\n    def do_POST(self):\n        print('method:', self.command)\n        print('path:', self.path)\n        print('HTTP version:', self.request_version)\n        print('headers:', self.headers)\n        content_length = int(self.headers['Content-Length'])\n        body = self.rfile.read(content_length).decode()\n        print('body:', body)\n\n        response = b'Roger!'\n        self.send_response(200)\n        self.send_header('Content-Length', len(response))\n        self.end_headers()\n        self.wfile.write(response)\n\n\nif __name__ == '__main__':\n    server = HTTPServer(('0.0.0.0', 8080), MyHandler)\n    server.serve_forever()\n要测试这个服务器，可以发送一个简单的 POST 请求：\nimport requests\n\nres = requests.post('http://localhost:8080/', data='Hello!')\nprint(res.text)\n在服务端应该会看到类似下面的输出：\nmethod: POST\npath: /\nHTTP version: HTTP/1.1\nheaders: Host: localhost:8080\nUser-Agent: python-requests/2.32.3\nAccept-Encoding: gzip, deflate\nAccept: */*\nConnection: keep-alive\nContent-Length: 6\n\n\nbody: Hello!\n而客户端则会收到如下的响应：\nRoger!\n下面对示例代码的关键部分进行更加详细地说明。\nHTTP 服务器的具体逻辑是通过继承BaseHTTPRequestHandler基类来实现的。当 HTTP 服务器收到 GET 请求时，就会调用对象的do_GET()方法，收到 POST 请求时会调用对象的do_POST()方法，以此类推。\nBaseHTTPRequestHandler基类提供了访问 HTTP 请求内容以及发送响应的接口。\nHTTP 请求的 request line 可以通过command、path和request_version属性来获取，它们都是字符串。请求的头部信息可以通过headers属性获取。headers的类型是http.client.HTTPMessage，是一个类似于dict的对象。通过读取rfile，可以读取请求的 body 部分。\n发送响应只需要往wfile中写入数据即可。发送 HTTP 响应的响应码和头部的时候，用send_response()和send_header()方法会更加方便，这些方法的底层实现依然是往wfile中写入数据，真正写入数据是在调用end_headers()的时候。\nhttp.server模块本身也提供了一个已经实现了的子类SimpleHTTPRequestHandler。通过在命令行输入python -m http.server 8080就可以快速启动一个使用了SimpleHTTPRequestHandler的 HTTP 服务器。\n","tags":[]},{"title":"用Python的zipfile标准库解压非标准编码的zip压缩包","date":"2024-07-19","url":"/posts/python-unzip-encoding","content":"时至今日（2024 年），依然有很多日本的 zip 压缩包采用 Shift JIS 编码。使用 7-Zip 等软件解压这些压缩包的时候，会发现文件名都是乱码。WinRAR 支持指定压缩包的文本编码，但是我个人不怎么喜欢 WinRAR。因此，我决定使用 Python 的标准库来解压非标准编码的 zip 压缩包。\n对于 3.11 及 3.11 之后的 Python 版本，ZipFile提供了一个metadata_encoding的参数，通过指定这个参数就可以设置编码：\nimport zipfile\n\nwith zipfile.ZipFile('foo.zip', metadata_encoding='cp932') as f:\n    f.extractall()\n如果是 3.11 之前的 Python 版本，对于非 UTF-8 编码的压缩包，ZipFile默认使用的是 cp437 编码，因此我们需要手动修改字符串编码，然后再解压：\nimport zipfile\nimport os\nimport shutil\n\nwith zipfile.ZipFile('foo.zip') as f:\n    for name in f.namelist():\n        path = name.encode('cp437').decode('cp932')\n        if path[-1] == '/':\n            os.makedirs(path, exist_ok=True)\n        else:\n            src = f.open(name)\n            os.makedirs(os.path.dirname(path), exist_ok=True)\n            with open(path, 'wb') as dst:\n                shutil.copyfileobj(src, dst)\n顺带一提，根据这个 StackOverflow 问题，zip 标准规定：若 UTF-8 的 flag 没有被设置，那么文件名的编码就应该是 cp437，但是实际实现很多都没有遵从标准。\n","tags":[]},{"title":"用PyTorch进行混合精度训练/推理","date":"2025-02-28","url":"/posts/pytorch-amp","content":"最近尝试了用混合精度的方法来加速模型训练。要用PyTorch进行混合精度训练非常简单，因为PyTorch已经封装好了这个功能，API也很便捷，要import的东西只有下面这些：\nfrom torch.amp import autocast, GradScaler\n要启用混合精度训练，对原有代码的改动也很少。首先是把前向传播的代码用autocast包裹起来：\nwith autocast('cuda'):\n    outputs = model(inputs)\n    loss = criterion(outputs, targets)\n然后在反向传播的时候，用GradScaler来缩放梯度：\nscaler = GradScaler()\n\n# In the training loop:\nscaler.scale(loss).backward()\nscaler.step(optimizer)\nscaler.update()\noptimizer.zero_grad()\n实验结果\n我在一台配备RTX 4090的机器上进行了实验，发现对于GPT-2，混合精度训练确实能够大幅提升训练速度，但是对于普通的全连接层或者卷积层，提升效果并不明显。我的解释是：Transformer模型的计算量远大于全连接层或者卷积层，这样才能体现出混合精度训练的优势。\n","tags":["神经网络"]},{"title":"PyTorch的BatchSampler的一个优化技巧","date":"2025-04-29","url":"/posts/pytorch-batch-trick","content":"最近在阅读PyTorch的源码的时候，发现torch.utils.data.sampler.BatchSampler的实现非常有意思：\n    def __iter__(self) -> Iterator[List[int]]:\n        # Implemented based on the benchmarking in https://github.com/pytorch/pytorch/pull/76951\n        sampler_iter = iter(self.sampler)\n        if self.drop_last:\n            # Create multiple references to the same iterator\n            args = [sampler_iter] * self.batch_size\n            for batch_droplast in zip(*args):\n                yield [*batch_droplast]\n        else:\n            batch = [*itertools.islice(sampler_iter, self.batch_size)]\n            while batch:\n                yield batch\n                batch = [*itertools.islice(sampler_iter, self.batch_size)]\n这段代码运用了zip和itertools.islice来实现批量采样的功能。我将其中的核心思想提取出来，写了一个简单的benchmark：\nimport time\nimport itertools\n\nsampler = range(320000)\nbatch_size = 32\n\n\ndef batch_iter_naive(sampler, batch_size):\n    batch = []\n    for i in sampler:\n        batch.append(i)\n        if len(batch) == batch_size:\n            yield batch\n            batch = []\n\n\ndef batch_iter_faster_1(sampler, batch_size):\n    sampler_iter = iter(sampler)\n    iters = [sampler_iter] * batch_size\n    for batch in zip(*iters):\n        yield list(batch)\n\n\ndef batch_iter_faster_2(sampler, batch_size):\n    sampler_iter = iter(sampler)\n    batch = [*itertools.islice(sampler_iter, batch_size)]\n    while batch:\n        yield batch\n        batch = [*itertools.islice(sampler_iter, batch_size)]\n\n\nstart = time.time()\nlist(batch_iter_naive(sampler, batch_size))\nprint(\"Naive method time:\", time.time() - start)\n\nstart = time.time()\nlist(batch_iter_faster_1(sampler, batch_size))\nprint(\"Faster method 1 time:\", time.time() - start)\n\nstart = time.time()\nlist(batch_iter_faster_2(sampler, batch_size))\nprint(\"Faster method 2 time:\", time.time() - start)\n结果发现改进后的方法确实比普通方法快了不少。\n至于速度变快的原因，我猜测是因为Python的for循环迭代开销太大，而zip和itertools.islice都是C实现的，迭代速度更快。从中可以总结出一个Python优化的技巧：用C实现的函数减少for循环的次数。\n","tags":[]},{"title":"使用screen命令在后台运行进程","date":"2024-04-26","url":"/posts/screen-background-process","content":"谈到在后台运行进程，大多数人第一个想到的可能是直接在命令后面加一个&，这么做有一些缺点：\n\n需要强行终止进程的时候，必须记住或者查询进程的名称、ID 等等。\n有时候使用&似乎会导致一个奇怪的 bug，使得终端输出紊乱甚至无法使用（参见这个 StackOverflow 问题）。\n\n因此，我发现使用 screen 命令是一种更好的解决方案。\n使用 screen 创建后台进程\n示例命令：\nscreen -dmS alice sleep 10\n这个命令中，sleep 10 是要执行的命令，-dm 表示在后台运行而不是进入 screen session，-S 表示我们要给这个 session 起一个名字，这里我起名为 alice。\n使用 screen 终止后台进程\n示例命令：\nscreen -S alice -X quit\n这个命令中，-S alice 指定了我们之前创建的名为 alice 的 session，而-X quit 表示让这个 session 执行 quit 命令（也就是退出）。\n","tags":[]},{"title":"screen命令常用操作","date":"2025-03-06","url":"/posts/screen-cheatsheet","content":"新建session\nscreen -S session_name\n列出所有session\nscreen -ls\n进入session\nscreen -r session_name\n暂时退出session\n首先按下Ctrl + A，然后按下D（代表Detach）。\n关闭session\n按下Ctrl + D或者输入exit。\n","tags":[]},{"title":"在脚本中使用相对路径的一种方法","date":"2024-12-11","url":"/posts/script-rel-path","content":"在软件项目中，经常需要写一些脚本来自动化执行一些任务。这里就有一个问题，脚本中的相对路径应该以什么为基准？目前我见过的常用方法有两种：\n\n\n以当前工作目录（CWD）为基准。这种方法是最简单直接的，但是缺点也很明显。首先是在终端执行脚本的时候，就必须在正确的目录下执行。其次就是当其他进程调用脚本时，也必须确保 CWD 设置正确。事实上，CWD 可以看作是操作系统提供的一个进程级别的全局变量。本身依赖于全局变量就不是一件非常优雅的事情。\n\n\n以脚本文件所在目录为基准。这种方法克服了上一种方法的缺点，但是却带来了一个新的问题：如果脚本文件被移动，那么脚本中的相对路径也要随之修改。\n\n\n那么，有没有一种不依赖于 CWD，又能允许脚本随意移动的方法呢？这就是 Git 所采用的方法：使用一个特殊的文件或者目录作为锚定点。在 Git 中，一个 Git 仓库的根目录下会有一个 .git 目录。当执行git命令的时候，Git 会从当前目录开始一层一层往上，找到第一个含有.git目录的目录作为仓库的根目录。\n我们可以借鉴这个思路，选取一个特殊的文件作为项目根目录的标记，例如.HERE_IS_ROOT，然后在脚本中寻找这个文件。下面是一个 Python 的例子：\nfrom pathlib import Path\n\nroot_dir = Path(__file__).resolve().parent\nwhile not (root_dir / '.HERE_IS_ROOT').exists():\n    root_dir = root_dir.parent\n","tags":[]},{"title":"Self-Instruct: 让LLM自己生成指令微调数据集","date":"2025-03-24","url":"/posts/self-instruct","content":"最近看到一篇比较有意思的论文：Self-Instruct: Aligning Language Models with Self-Generated Instructions。这篇论文的思路就是让LLM自己生成指令微调数据集，然后用它反过来训练自己。这么做有点左脚踩右脚的感觉。基于这个思路，斯坦福大学的团队训练了一个名为Alpaca的模型，模型的权重、数据集和代码都在GitHub上开源了：tatsu-lab/stanford_alpaca。下面就简单介绍一下这个方法。\n总体流程\nSelf-Instruct的总体流程可以用下面的伪代码来表示：\n初始化数据集\n\nwhile True:\n    从数据集中随机采样instruction，生成新的instruction\n    判断instruction是否为分类任务\n    if 是:\n        用输出优先（output-first）的方法生成input和output\n    else:\n        用输入优先（input-first）的方法生成input和output\n    评估生成的数据质量，过滤掉质量差的数据\n    将生成的新数据加入数据集\n这里需要详细解释的步骤有以下几个：\n\n如何用采样的instruction生成新的instruction。\n如何判断instruction是否为分类任务。\n如何生成input和output。\n\n生成instruction\n这个步骤首先是要从数据集中随机采用一些instruction，然后构造以下的prompt：\n请给出一系列的任务：\n\n任务1：{任务1的描述}\n任务2：{任务2的描述}\n...\n任务n：{任务n的描述}\n任务n+1：\n例如：\n请给出一系列的任务：\n\n任务1：给出一个句子，判断这个句子的情感是正面还是负面。\n任务2：将下面的段落翻译成中文。\n...\n任务n：给出一个一元一次方程，求解这个方程。\n任务n+1：\n判断是否为分类任务\n这个步骤比较简单，直接构造prompt询问LLM即可：\n下面的任务是分类任务吗？\n\n任务：{任务1的描述}\n答案：{是/否}\n\n任务：{任务2的描述}\n答案：{是/否}\n\n...\n\n任务：{任务n的描述}\n答案：{是/否}\n\n任务：{任务n+1的描述}\n答案：\n生成input和output\nSelf-Instruct会根据instruction是否为分类任务来采取不同的方式。如果是分类任务，就用output-first的方式生成input和output；如果不是分类任务，就用input-first的方式生成input和output。\nOutput-first\nOutput-first的方式是先让LLM给出可能的output，然后再让LLM生成input。这个过程可以用下面的prompt来表示：\n给出一个分类任务，生成所有可能的标签及其对应的输入：\n\n任务：{任务1的描述}\n输出：{任务1的标签1}\n输入：{任务1的标签1对应的输入}\n输出：{任务1的标签2}\n输入：{任务1的标签2对应的输入}\n...\n输出：{任务1的标签n}\n输入：{任务1的标签n对应的输入}\n\n...\n\n任务：{任务n的描述}\nInput-first\nInput-first的方式就很直接了，直接给出instruction，然后让LLM生成input和output：\n给出一个任务，生成尽可能多的输入和输出：\n\n任务：{任务1的描述}\n输入：{任务1的输入1}\n输出：{任务1的输出1}\n输入：{任务1的输入2}\n输出：{任务1的输出2}\n...\n输入：{任务1的输入n}\n输出：{任务1的输出n}\n\n...\n\n任务：{任务n的描述}\n其实input-first和output-first没什么本质的区别，只是prompt模板给LLM的引导不同。\n","tags":[]},{"title":"关于SysVinit和systemd","date":"2024-09-14","url":"/posts/service-systemctl","content":"Linux 管理服务的系统有两种：SysVinit 和 systemd。systemd 比 SysVinit 推出得更晚。SysVinit 本质上就是运行一些管理服务的 shell 脚本，轻量灵活但是简陋，而 systemd 则是一个封装得更加复杂的系统。现在大多数 Linux 发行版都已经使用 systemd 了，但是在 Docker 环境下，由于种种原因，大多数情况下还是会使用 SysVinit。因此，同时掌握这两种系统还是很有必要的。\n用 systemd 管理服务\nsystemd 管理服务的命令是systemctl。\n启动或关闭某个服务：\nsudo systemctl start $SERVICE\nsudo systemctl stop $SERVICE\n查看某个服务的状态：\nsudo systemctl status $SERVICE\n管理服务的开机启动：\nsudo systemctl enable $SERVICE\nsudo systemctl disable $SERVICE\n用 SysVinit 管理服务\n使用service命令启动或关闭某个服务：\nsudo service $SERVICE start\nsudo service $SERVICE stop\n下面介绍如何在 SysVinit 下管理服务的开机启动。由于 SysVinit 大多是在 Docker 环境下使用，而在 Docker 环境下其实没有所谓开机启动的概念，所以下面的内容可以跳过。\n使用update-rc.d命令开机启动某个服务：\nsudo update-rc.d $SERVICE defaults\n关闭开机启动：\nsudo update-rc.d -f $SERVICE remove\n","tags":[]},{"title":"用CloudFlare Workers建立免费的Docker Hub镜像","date":"2024-10-31","url":"/posts/set-up-docker-hub-mirror","content":"2024 年 12 月 28 日更新：似乎有消息称 CloudFlare 更新了用户条款，禁止将其服务用于代理用途。因此这个方法可能有风险。\n由于中国封锁了 Docker Hub，导致在国内用 Docker 的时候会遇到很多问题。一个最简单的办法就是使用镜像，然而很遗憾的是，很多镜像站也被封锁了，因此我不得不自己建立一个 Docker Hub 镜像。在 GitHub 上，我发现一个使用 CloudFlare Workers 建立 Docker Hub 镜像的项目。CloudFlare Workers 是有免费版的，这就意味着可以免费建立一个 Docker Hub 镜像。然而原项目的 README 文档写得实在是太简陋了，我摸索了很久才最终测试成功，因此就将一个更详细的步骤写在这里。\n首先当然是要注册一个 CloudFlare 账号，然后点击原项目 README 中的“Deploy with Workers”的按钮，按照网页的提示操作。操作完成之后，应该会：\n\n\n在 CloudFlare 里创建 2 个分别名为cloudflare-docker-proxy和cloudflare-docker-proxy-staging，后者不知道是做什么用的，也不用管，最终访问的是前者。\n\n\n在你授权的 GitHub 账户下自动 fork 一个仓库。需要修改代码的时候就在自己的仓库下面修改，修改完之后 GitHub Actions 会自动部署更改。\n\n\n接下来要处理一下域名的问题。默认情况下 worker 绑定的域名是cloudflare-docker-proxy.<username>.workers.dev，但很遗憾的是，workers.dev 这个域名也被墙了，所以我们需要配置一个自己的域名。首先要在 CloudFlare dashboard 的“Add a domain”界面把你的域名添加上去。如果你的域名不是在 CloudFlare 注册的，添加完之后还需要在域名注册商那里将 DNS 服务器改成 CloudFlare 指定的 DNS 服务器。之后看到状态是“Active”就算成功了。在进行下一步之前，要将 CloudFlare 上的 DNS 记录全部删掉。\n然后在cloudflare-docker-proxy worker 的管理界面的“Settings”-“Domains & Routes”中把你的域名作为 custom domain 添加进去。\n最后就是修改代码。把src/index.js中的routes修改为你的域名即可：\nconst dockerHub = \"https://registry-1.docker.io\";\n\nconst routes = {\n  \"foo.com\": dockerHub,\n};\n最后试着用镜像源 pull 一下镜像：\ndocker pull foo.com/hello-world\n在 Docker image 的名字前面加上镜像域名只是一个临时的办法，要让镜像永久生效，可以参考这篇文章。\n","tags":[]},{"title":"用setuptools打包资源文件","date":"2025-01-13","url":"/posts/setuptools-package-data","content":"默认情况下，setuptools 只会打包 Python 源代码文件，但有时候我们还需要打包一些资源文件，比如配置文件、模板文件、图片等等。这篇文章将会介绍如何用 setuptools 打包资源文件，并在库执行的时候访问这些文件。\n我们假设项目的目录结构如下：\nmyproject/\n├── mypkg/\n│   ├── __init__.py\n│   └── data/\n│       ├── template.txt\n└── setup.py\n指定要打包的资源文件\n要打包 mypkg/data/template.txt ，我们需要在 setup.py 中添加如下代码：\nfrom setuptools import setup\n\nsetup(\n    # ...\n    include_package_data=True,\n    package_data={'mypkg': ['data/template.txt']},\n    # ...\n)\npackage_data参数也可以使用 glob 模式，比如mypkg': ['data/*.txt']。\n访问资源文件\n可以使用importlib.resources模块来访问资源文件。这个模块是在 Python 3.7 中引入的。\n使用open_text或者open_binary方法来打开资源文件：\nimport mypkg\nimport importlib.resources\n\nanchor = mypkg  # 也可以是字符串'mypkg'\nwith importlib.resources.open_text(anchor, 'data/template.txt') as f:\n    print(f.read())\n","tags":[]},{"title":"setuptools的setup()函数的常用参数","date":"2024-12-28","url":"/posts/setuptools-setup-options","content":"本文将介绍setuptools的setup()函数的常用参数，主要参考的是官方文档。\nname\n包的名称，要将下划线替换为短横线，例如my-package。\nversion\n版本号，例如1.2.3。\ndescription\n包的简要描述。\nlong_description\n包的详细描述，通常是从一个文件中读取的。\nlong_description_content_type\nlong_description的类型，例如text/markdown。\nauthor\n包的作者。\nauthor_email\n作者的邮箱。\nurl\n包的主页，例如 GitHub 地址。\npackages\n包含的包，通常不会手动填写，而是用find_packages()函数来查找。\npackage_dir\n包的目录，例如{'': 'src'}。\nclassifiers\n一个 list，包含包的分类信息，例如Programming Language :: Python :: 3。可选的分类信息可以在PyPI上查看。\nlicense\n包的许可证，例如MIT。\ninstall_requires\n运行时依赖的包的 list。\nentry_points\n一个 dict，包含了包的入口点，例如{'console_scripts': ['my-command = my_package.my_module:main']}。假如安装了这个包，就可以通过命令行执行my-command命令，这个命令会调用my_package.my_module模块的main()函数。\npython_requires\n指定 Python 的版本，例如>=3.6。\n示例\nfrom setuptools import setup, find_packages\n\nsetup(\n    name='my-package',\n    version='1.2.3',\n    description='A short description',\n    long_description=open('README.md').read(),\n    long_description_content_type='text/markdown',\n    author='Author Name',\n    author_email='xxx@example.com',\n    url='https://xxx.com',\n    packages=find_packages('src'),\n    package_dir={'': 'src'},\n    classifiers=[\n        'Programming Language :: Python :: 3',\n    ],\n    license='MIT',\n    install_requires=[\n        'requests',\n    ],\n    entry_points={\n        'console_scripts': [\n            'my-command = my_package.my_module:main',\n        ],\n    },\n    python_requires='>=3.6',\n)\n","tags":[]},{"title":"在shebang中使用/usr/bin/env","date":"2024-05-11","url":"/posts/shebang-usr-bin-env","content":"假设我们要编写一个 bash 脚本，很多人会这么写 shebang：\n#!/bin/bash\n这么做相当于是指定了绝对路径，但有时候，我们希望操作系统在 PATH 中寻找程序，这时候就可以利用/usr/bin/env：\n#!/usr/bin/env bash\n","tags":[]},{"title":"推荐一个免费的SMTP送信服务：SMTP2GO","date":"2025-03-31","url":"/posts/smtp2go","content":"最近我想实现这么一个功能：编写一个程序监控股价，当剧烈波动时，就给我发个推送通知。仔细想了想，发现成本最低、实现最简单的推送方法是就是给我的邮箱发个邮件。但很可惜的是，现在主流的邮箱服务商都启用了2FA，因此不能直接通过用户名和密码来发送邮件了。最后我找到了一个名为SMTP2GO的服务商，它们提供的SMTP服务可以直接使用用户名和密码来发送邮件。下面是利用Python和SMTP2GO来发送邮件的教程。\n首先要注册一个SMTP2GO的账户。注意：SMTP2GO不支持用public domain的邮箱（例如outlook.com等）注册，必须通过企业、学校邮箱之类的注册。\n注册之后，在控制台的\"Sending\" -> \"Verified Senders\" -> \"Single sender emails\"里添加一个发件人邮箱。这个邮箱可以是任意邮箱，不一定是你注册SMTP2GO时用的邮箱。当你发送邮件的时候，收件人看到的发件人就是这个邮箱。\n然后在\"Sending\" -> \"SMTP Users\"里添加一个SMTP用户。用户名就是你刚刚添加的发件人邮箱，密码自己设置。\n然后就可以用Python通过SMTP发送邮件了，下面是一段示例代码：\nimport smtplib\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\n\nsmtp_server = \"mail.smtp2go.com\"\nsmtp_port = 2525\nsmtp_user = \"<替换为你的用户名>\"\nsmtp_password = \"<替换为你的密码>\"\nrecipient_email = \"<替换为收件人邮箱>\"\nsubject = \"邮件的标题\"\nbody = \"邮件内容\"\n\nmsg = MIMEMultipart()\nmsg[\"From\"] = smtp_user\nmsg[\"To\"] = recipient_email\nmsg[\"Subject\"] = subject\nmsg.attach(MIMEText(body, \"plain\"))\n\nwith smtplib.SMTP(smtp_server, smtp_port) as server:\n    server.starttls()\n    server.login(smtp_user, smtp_password)\n    server.sendmail(smtp_user, recipient_email, msg.as_string())\n","tags":[]},{"title":"将本地机器的SSH密钥添加到远程机器的方法","date":"2024-11-13","url":"/posts/ssh-copy-id","content":"用密码进行 SSH 登入麻烦又不安全。这篇文章将会教你如何将本地机器的 SSH 密钥添加到远程机器，用密钥快速登入 SSH。\n首先，如果是 Windows 系统，建议安装下 Git for Windows，哪怕你不用 Git。因为 Git for Windows 附带了 SSH 客户端工具，以及兼容 Linux 的 Git Bash。\n如果你的电脑上还没有 SSH 密钥，就输入下面的命令生成一个：\nssh-keygen -t ed25519 -C \"your_email@example.com\"\n然后运行下面的命令将本机密钥复制到远程机器上（Widnows 下面要在 Git Bash 中运行，因为ssh-copy-id不是 Windows 可执行文件）：\nssh-copy-id username@remote_host\n运行命令的时候，会让你输入密码登入远程机器。\nssh-copy-id命令还有两个常用的参数：\n\n-i：可以用来指定密钥文件的路径。\n-f：默认情况下，ssh-copy-id需要同时提供私钥和公钥。如果只有公钥，就需要加上这个参数。\n\n例如：\nssh-copy-id -i ~/.ssh/id_rsa.pub -f username@remote_host\n","tags":[]},{"title":"Windows下用HTTP代理连接SSH","date":"2024-04-23","url":"/posts/ssh-http-proxy-windows","content":"这里假设使用的是 Git 提供的 SSH。首先打开 Git Bash，输入命令\nwhere connect\n查找 connect.exe 的路径，这里假设是 C:\\Program Files\\Git\\mingw64\\bin\\connect.exe。\n然后打开 SSH 的配置文件，新增如下内容（将{hostname}:{port}替换成代理服务器地址）：\nHost *\n    ProxyCommand \"C:\\Program Files\\Git\\mingw64\\bin\\connect.exe\" -H {hostname}:{port} %h %p\n这样就可以让所有 SSH 连接都经过代理服务器了。\n注意，这里 connect.exe 一定要使用绝对路径，哪怕将所在目录添加到了 PATH 中也不行。\n","tags":[]},{"title":"用sudo命令切换至root用户的正确方式","date":"2024-12-10","url":"/posts/sudo-s","content":"最近遇到了一个问题：我想查看一个只有 root 用户才有权限访问的目录下有哪些文件，于是我尝试用sudo cd /path/to/dir命令切换当前目录，结果报错。后来才知道cd并不是一个真实存在的可执行文件，而是一个 Shell 内置的特殊命令。所以，要通过cd切换目录，就只能把当前 Shell 的用户切换成 root 才行。\n那么，将当前 Shell 的用户切换成 root，应该怎么做呢？我在网上看到了许多方法，包括sudo su、sudo -s等等。这些命令都能奏效，那么到底应该用哪个呢？我看到 StackOverflow 上有一篇回答总结得很好：\n推荐使用的命令是sudo -s和sudo -i。它们的区别在于，前者会保留原用户的环境变量，而后者会重置环境变量，将环境变量初始化为 root 刚登入的状态。除此之外的命令都不推荐使用。\n","tags":[]},{"title":"关于sudo的secure_path设置","date":"2024-12-11","url":"/posts/sudo-secure-path","content":"sudo 有一个名为secure_path的设置，从 sudo 1.9.16 开始，secure_path设置会默认启用。官方博客介绍了这个改动的原因。\n当使用 sudo 执行命令的时候，用户当前的环境变量会被修改。其中PATH会被设置为secure_path的值。因此，经常会发生加了sudo之后就找不到命令的情况。解决方法也是非常的简单，要么修改secure_path的值，要么直接删除这个设置，这样 sudo 就不会修改PATH了。\n运行sudo visudo，然后删除或者修改下面这一行即可。\nDefaults        secure_path=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin\"\n还有一种方法是使用绝对路径来运行命令，可以用which命令来查看命令的绝对路径。下面是一个例子。\nsudo $(which COMMAND)\n","tags":[]},{"title":"1932-1939年的台语歌曲","date":"2024-05-15","url":"/posts/taiwanese-songs-1932-1939","content":"最近阅读了厦门大学出版社出版的《传播与流变: 海峡两岸闽南语歌曲研究》，发现 91 页有一个表格，列举了所有 1932-1939 年的台语歌曲。于是我便在 YouTube Music 上建立了一个歌单，收集了表格中的经典台语歌曲。由于 YouTube Music 音源有限，因此遗漏了一些歌曲。这些缺失的歌曲可以在台湾声音一百年里面找到。\n下面是原表格：\n\n\n\n\n","tags":[]},{"title":"BT种子文件和磁力链的区别","date":"2024-06-16","url":"/posts/torrent-file-vs-magnet-url","content":"在 BT 分享网站上，经常可以看见一个资源有两种下载方式：BT 种子文件或者磁力链。使用这两种方式有什么区别呢？先下结论：磁力链只存储了 BT 种子文件部分内容的 hash 值。使用磁力链下载文件的时候，要首先从拥有资源的节点上根据 hash 值获得 BT 种子文件，然后才能下载。因此可以说磁力链只是一个中间过程，BT 种子文件才是最终的结果。\n接下来将首先介绍使用 BT 协议下载文件的时候需要的信息，然后将分别介绍 BT 种子文件和磁力链分别存储了哪些信息。\nBT 协议需要的信息\n要让 BT 协议工作，需要两个最关键的信息。第一个是 tracker 服务器的地址。第二个是文件名及其 hash 值。Tracker 服务器的作用是让需要下载资源的节点能够发现拥有资源的节点。在这两个信息中，文件名及其 hash 值显然是更重要的，因为没有这个信息，我们就无法确定要下载的文件。至于 tracker 服务器地址这个信息其实并不是那么重要，因为 tracker 服务器并不是唯一且固定的，知名的 tracker 服务器列表在互联网上都可以查到，所以我们直接让 BT 客户端使用互联网上的 tracker 列表就行。\nBT 种子文件\nBT 种子文件本质上是一个使用某种特定编码的字典。这里的字典是指能够存储键值对的数据结构。在这个字典中有两个关键的字段：announce 和 info。annouce 字段中存储了 tracker 服务器的地址，info 字段中则存储了文件名及其 hash 值。\n磁力链\n磁力链本质上也是存储了若干键值对，其格式如下：\nmagnet:?{key1}={value1}&{key2}={value2}&{key3}={value3}...\n常见的 key 有两个：tr、dn 和 xt。tr 用于指定 tracker 服务器地址。dn 用于指定该资源的名称，这个名称仅仅只是为了方便用户识别，理论上可以随意指定。xt 则存储了 BT 种子文件中 info 字段的 hash 值。通过这个 hash 值，就可以在拥有该资源的节点上获取对应的 BT 种子。但很显然，如果找不到拥有该资源的节点，就无法通过磁力链获取文件的具体信息。\n参考资料\n\nhttps://en.wikipedia.org/wiki/Torrent_file\nhttps://en.wikipedia.org/wiki/Magnet_URI_scheme\n\n","tags":[]},{"title":"Transformer decoder推理时是否应该设置causal mask","date":"2025-02-19","url":"/posts/transformer-inference-mask","content":"最近在扣关于Transformer的细节，结果发现了一个问题：众所周知，在训练Transformer的过程中，decode的时候要使用causal mask避免泄漏还未生成的信息。在推理的时候，由于我们是逐个生成token的，所以不会出现泄漏的问题，那么是不是就不需要causal mask了呢？后来我看到StackExchange上有个人和我有相同的问题：Is the Mask Needed for Masked Self-Attention During Inference with GPT-2。目前我对这个问题的理解是这样的：答案是依然需要。\n其原因很简单。如果我们只看单层的Transformer，由于在生成的时候只用到最后一个token的输出，所以确实似乎不需要causal mask。但是在多层的Transformer中，每一层的输出都会作为下一层的输入，因此前面几层的每一个token的输出都会被用到。如果我们不使用causal mask，就会导致前面几层的输出会发生未来信息的泄漏，这样就导致推理的过程和训练的过程逻辑不一致，从而会影响模型的性能。\n不过这个问题也不需要我们调库的人操心，因为transformers库提供的模型类都会自动加上causal mask，不管是在训练还是推理的时候。\n","tags":["transformer"]},{"title":"Ubuntu常用软件包","date":"2024-06-23","url":"/posts/ubuntu-common-packages","content":"下面列出的软件包主要是命令名称和包名不一致的软件包。\n\n\n\n包名\n提供的命令\n\n\n\n\nbuild-essential\ngcc, g++, make\n\n\niproute2\nip\n\n\niputils-ping\nping\n\n\n\n","tags":[]},{"title":"Ubuntu安装PostgreSQL的教程","date":"2024-11-20","url":"/posts/ubuntu-install-pgsql","content":"安装\n使用包管理器就可以直接安装 PostgreSQL：\nsudo apt install postgresql\n配置\n创建新用户和数据库\n默认情况下，PostgreSQL 会创建一个名为postgres的数据库超级用户以及一个同名的 Unix 用户（注意区分数据库用户和 Unix 用户的概念）。postgres数据库用户没有密码，只有在主机上登入postgres Unix 用户之后，才有权限登入postgres数据库用户。psql是 PostgreSQL 的客户端命令行工具。输入下面的命令就可以通过postgres数据库用户来操纵数据库。\nsudo -u postgres psql\n然后输入以下命令，就可以创建一个名为user1的用户和名为db1的数据库，并赋予user1操纵db1的所有权限。\nCREATE USER user1 WITH PASSWORD 'your_password';\nCREATE DATABASE db1;\nGRANT ALL PRIVILEGES ON DATABASE db1 TO user1;\n\\c db1 postgres\nGRANT ALL ON SCHEMA public TO user1;\n\\q\n顺带一提，可以通过pwgen命令生成一个随机的 16 位强密码：\nsudo apt install pwgen\npwgen -s 16 1\n允许外网通过密码连接\n默认配置下，PostgreSQL 不允许外网连接通过密码登入数据库。下面介绍如何解除该限制。\n首先编辑/etc/postgresql/*/main/postgresql.conf（这里的*代表 PostgreSQL 的版本号，下同），加入这一行：\nlisten_addresses = '*'\n然后在/etc/postgresql/*/main/pg_hba.conf中加入这一行：\nhost  all  all  0.0.0.0/0  scram-sha-256\n你可以通过下面的命令快速编辑配置文件：\necho \"listen_addresses = '*'\" | sudo tee -a /etc/postgresql/*/main/postgresql.conf\necho \"host  all  all  0.0.0.0/0  scram-sha-256\" | sudo tee -a /etc/postgresql/*/main/pg_hba.conf\n将数据库文件移动至数据盘中\n默认情况下，PostgreSQL 的数据库文件存储在/var/lib/postgresql/*/main/下面。可以通过软链接将数据库文件迁移到别的目录。此外要注意定期备份。\n启动\n输入以下命令即可启动 PostgreSQL，并设置开机自动启动：\nsudo service postgresql start\nsudo service postgresql enable\n这里有一个坑，就是postgresql并不是一个真实存在的 service。真正的 PostgreSQL service 名称是带版本号的。postgresql这个 service 只不过是重定向至真正的 service 而已。因此，如果你输入sudo service postgresql status，那么不管 PostgreSQL 有没有成功启动，你看到的状态都会是active (exited)。要想看到真正的运行状态，就要输入：\nsudo service postgresql* status\n","tags":[]},{"title":"Ubuntu安装Redis","date":"2025-03-17","url":"/posts/ubuntu-install-redis","content":"安装Redis\n在Ubuntu里，可以直接通过apt安装Redis：\nsudo apt-get install lsb-release curl gpg\ncurl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg\nsudo chmod 644 /usr/share/keyrings/redis-archive-keyring.gpg\necho \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list\nsudo apt-get update\nsudo apt-get install redis\n安装完成后，会自动启动Redis，并设置为开机启动。\n可以通过redis-cli命令来测试Redis是否安装成功：\n$ redis-cli\n127.0.0.1:6379> ping\nPONG\n允许网络连接\n默认情况下，Redis只允许本地连接。如果需要允许通过网络连接，可以修改/etc/redis/redis.conf文件，将bind属性修改为允许的IP地址。\n默认情况下，如果通过网络连接，Redis会强制要求启用密码。如果是在内网环境下，不启用密码其实也可以。可以通过在配置文件中禁用protected-mode来允许无密码连接：\nprotected-mode no\n修改完配置文件后，需要重启Redis使配置生效：\nsudo systemctl restart redis\n用Python连接Redis\n可以通过redis库来连接Redis：\npip install redis\nimport redis\n\nr = redis.Redis(host='localhost', port=6379, db=0)\n\ntry:\n    r.ping()\n    print('Connected to Redis!')\nexcept redis.ConnectionError:\n    print('Could not connect to Redis.')\nRedis持久化\n默认情况下，Redis的持久化文件会保存在/var/lib/redis/dump.rdb中。建议定时备份该文件。\n","tags":[]},{"title":"实用软件推荐","date":"2024-05-21","url":"/posts/useful-software","content":"二进制编辑器、PE 文件解析器等\nHexEd.it：一个在线二进制编辑器。\nPE-bear：开源的 PE 文件解析器。\nResource Hacker：编辑、提取 PE 文件的资源（.rsrc section）。\n","tags":[]},{"title":"允许UWP应用访问localhost","date":"2024-05-12","url":"/posts/uwp-localhost","content":"由于 Windows 的安全限制，UWP 应用默认是不能访问 localhost 的。这会导致 UWP 应用无法翻墙。要允许 UWP 应用访问 localhost，就必须修改注册表。GitHub 上有人编写了一个 GUI 程序，可以帮助我们修改注册表来解除限制。原作者的仓库地址在这里。可惜的是，原作者只提供了源代码，而没有提供编译好的可执行文件。不过好在 GitHub 上有人 fork 了仓库，提供了编译好的可执行文件。\n","tags":[]},{"title":"主机与VirtualBox虚拟机的网络通信配置","date":"2024-03-15","url":"/posts/virtualbox-network","content":"虚拟机连接主机\nVirtualBox 已经默认建立了一个网卡，可以通过这个网卡和主机通信。\n首先点击菜单栏的“管理”->“工具”->“网络管理器”。\n\n然后就可以在“仅主机(Host-Only)网络”里看到网卡的 IP 地址。图中所示的地址是 192.168.56.1。\n\n主机连接虚拟机\nVirtualBox 默认的 NAT 网络只能支持虚拟机与主机或外网通信，主机是无法与虚拟机通信的。有时候我们需要让主机与虚拟机通信，比如让主机连接虚拟机的 SSH 等等。这种情况下，我们需要手动配置 VirtualBox 的端口转发。\n首先打开虚拟机的设置。\n\n把虚拟机网卡的连接方式改为“NAT 网络”（注意不是默认的“网络地址转换(NAT)”！）。网络名称用默认已经建立好的“NatNetwork”就可以。\n\n然后点击菜单栏的“管理”->“工具”->“网络管理器”。\n\n最后在 NAT 网络的端口转发里新增一条转发规则即可。这里我增加的规则是将 localhost:6312（主机）转发到 10.0.2.15:22（虚拟机）。10.0.2.15 是虚拟机的 IP。\n\n","tags":[]},{"title":"Vite配置开发/生产环境下的环境变量","date":"2024-09-20","url":"/posts/vite-env-var","content":"在vite.config.js里面有一个envDir的选项，用来指定 Vite 加载的环境变量的路径，默认值是项目的根目录，可以改为其他值：\nexport default defineConfig({\n  envDir: \"env\",\n});\n在envDir的目录下新建三个文件：.env、.env.development和.env.production。其中.env是任何模式下都会加载的环境变量，.env.development只会在 development 模式下加载，而.env.production只会在 production 模式下加载。development 和 production 是 Vite 预定义的两个模式，你也可以定义自己的模式，比如 test，这样文件名就是.env.test。\ndevelopment 是 Vite 在开发模式下的默认模式，而 production 是 Vite build 时的默认模式。也可以用--mode参数来手动设定模式。也就是说：\n# 这两个命令是等价的\nvite\nvite --mode development\n\n# 这两个命令是等价的\nvite build\nvite build --mode production\n.env 文件的格式是简单的 key=value 的格式，其中 value 可以加引号，也可以不加，比如：\nkey1=value1\nkey2=\"value2\"\n为了防止机密信息泄露，Vite 默认只会导出以VITE_开头的变量。\n要访问环境变量，可以在 JS 中使用import.meta.env.VITE_XXX来访问。需要注意，import.meta只能在 JS Module 的环境中使用，因此不能直接在 Vue 的 template 中访问，需要在 script 部分中先加载才行：\n<script setup>\nconst XXX = import.meta.env.VITE_XXX;\n</script>\n\n<template>\n  <p>{{ XXX }}</p>\n</template>\n","tags":[]},{"title":"在Vite项目中使用Tailwind CSS","date":"2024-12-19","url":"/posts/vite-tailwind-css","content":"本文介绍如何在现有的 Vite 项目中使用 Tailwind CSS。\n首先要安装 Tailwind CSS：\nnpm install -D tailwindcss postcss autoprefixer\n然后初始化配置文件：\nnpx tailwindcss init -p\n这将生成两个文件：tailwind.config.js和postcss.config.js。\n接下来在tailwind.config.js中配置content选项。只有content中指定的文件才可以使用 Tailwind CSS。例如：\n/** @type {import('tailwindcss').Config} */\nexport default {\n  content: [\"./index.html\", \"./src/**/*.{js,jsx,ts,tsx,vue}\"],\n  theme: {\n    extend: {},\n  },\n  plugins: [],\n}\n要让 Tailwind CSS 生效，还需要让网页导入 Tailwind CSS 的样式。比如说，可以在全局的 CSS 文件中加入以下内容：\n@tailwind base;\n@tailwind components;\n@tailwind utilities;\n到这里就配置完成了。我们不需要修改 Vite 的配置文件，因为 Tailwind CSS 是基于 PostCSS 框架的，而 Vite 会自动检测并启用 PostCSS 插件。\n","tags":[]},{"title":"VS Code + CMake + Windows配置指南","date":"2025-03-27","url":"/posts/vscode-cmake-windows","content":"这篇指南会教你如何在Windows下给VS Code配置一个舒服的CMake开发环境。\n首先要安装VS Build Tools，可以在VS官网下载。\n然后给VS Code安装CMake Tools和clangd扩展。\n然后在VS Code的配置文件里加入以下设置：\n\"cmake.generator\": \"Ninja\"\n之所以要使用Ninja，是因为默认的VS生成器无法生成clangd需要的compile_commands.json文件。\n这样就配置完成了。\n如果发现编译器输出的信息有乱码，可以在VS Code的配置文件里加入以下设置：\n\"cmake.outputLogEncoding\": \"utf-8\"\n如果utf-8不行就改成gbk。\n","tags":[]},{"title":"关于VSCode的launch.json","date":"2024-07-20","url":"/posts/vscode-debugging-config","content":"VSCode 工作区的.vscode/launch.json文件中存储着调试和运行的配置。\n基本结构\nlaunch.json的基本结构如下：\n{\n  \"version\": \"0.2.0\",\n  \"configurations\": [\n    {\n      \"name\": \"foo\",\n      \"type\": \"debugpy\",\n      \"request\": \"launch\",\n      \"program\": \"${workspaceFolder}/bar.py\"\n    }\n    // ...\n  ]\n}\n首先有一个version字段指定配置文件的版本，然后在configurations数组中指定调试或运行的参数。\n必须参数\n每一个启动项都必须有这 4 个参数：\n\nname：显示在 VSCode 界面上的启动项名称。\ntype：要使用的 debugger。debugger 都是由插件提供的。\nrequest：只能是launch或者attach。launch表示调试时由 VSCode 启动一个新的进程，attach则表示让 debugger 连接到现有的进程。一般都是选择launch。\nprogram：程序的入口文件。\n\ndebugpy 的参数\ncwd参数可以指定运行脚本时的工作目录，默认值是${workspaceFolder}。\njustMyCode参数的默认值是true，表示不调试不在当前工作区内的代码（比如标准库、第三方库），建议设置为false。\n切换调试和运行模式\n在 VSCode 里，调试和运行是共享配置的。按下 F5 会开始调试当前的选中项，按下 Ctrl+F5 则会以运行模式（不启动 debugger）启动当前的选中项。\n同时调试多个对象\n假设有server.py和client.py两个程序需要同时启动，只需要这么编写.vscode/launch.json即可：\n{\n  \"version\": \"0.2.0\",\n  \"configurations\": [\n    {\n      \"name\": \"Client\",\n      \"type\": \"debugpy\",\n      \"request\": \"launch\",\n      \"program\": \"${workspaceFolder}/client.py\"\n    },\n    {\n      \"name\": \"Server\",\n      \"type\": \"debugpy\",\n      \"request\": \"launch\",\n      \"program\": \"${workspaceFolder}/server.py\"\n    }\n  ],\n  \"compounds\": [\n    {\n      \"name\": \"Server/Client\",\n      \"configurations\": [\"Server\", \"Client\"],\n      \"stopAll\": true\n    }\n  ]\n}\n接着运行名为Server/Client的启动项即可同时启动Server和Client两个调试任务。启用stopAll属性之后，一旦关闭正在调试的任何一个程序，就会将其他程序也一并关闭。\n","tags":[]},{"title":"判断Windows的lib文件是静态库还是动态库的方法","date":"2025-03-27","url":"/posts/windows-lib-static-or-dynamic","content":"众所周知，在Linux下，动态库的后缀名是.so，而静态库的后缀名则是.a。但是在Windows下，静态库的后缀名是.lib，而动态库则包含两个文件，一个是.dll文件，另一个是.lib文件。下面的表格总结了Linux和Windows的差异：\n\n\n\n类型\nLinux\nWindows\n\n\n\n\n静态库\n.a\n.lib\n\n\n动态库\n.so\n.dll + .lib\n\n\n\n下表是Linux使用静态库和动态库的过程：\n\n\n\n类型\n编译时\n运行时\n\n\n\n\n静态库\n链接.a文件\n无需额外依赖\n\n\n动态库\n链接.so文件\n依赖.so文件\n\n\n\n下表是Windows使用静态库和动态库的过程：\n\n\n\n类型\n编译时\n运行时\n\n\n\n\n静态库\n链接.lib文件\n无需额外依赖\n\n\n动态库\n链接.lib文件\n依赖.dll文件\n\n\n\n所以说在Windows下面，一个.lib文件不一定是静态库，那么，我们如何判断一个.lib文件是静态库还是动态库呢？\n我们可以用VS构建工具包中的lib命令来判断，只要输入下面的命令：\nlib /list xxx.lib\n如果是静态库，那么输出的结果应该是若干.obj文件，如果是动态库，那么输出的结果应该是若干.dll文件。\n","tags":[]},{"title":"Windows NTP指南","date":"2024-12-05","url":"/posts/windows-ntp","content":"本文将介绍如何在 Windows 系统上启动 NTP 服务器，以及如何让 NTP 客户端同步时间。\n注意：本文中的大部分操作都需要管理员权限。\n防火墙配置\n默认配置下，Windows 防火墙会阻止所有未经允许的入站连接。因此，我们需要允许 NTP 服务（使用 UDP 123 端口）的入站连接。\nNew-NetFirewallRule -DisplayName \"Allow NTP\" -Direction Inbound -Protocol UDP -LocalPort 123 -Action Allow\n启动 NTP 服务器\n默认配置下，Windows 的 NTP 服务（w32time）是禁用服务器功能的，我们需要手动修改注册表，然后重启服务来开启：\nSet-ItemProperty -Path \"HKLM:\\SYSTEM\\CurrentControlSet\\Services\\W32Time\\TimeProviders\\NtpServer\" -Name \"Enabled\" -Value 1\nRestart-Service w32time\n然后运行下面的命令让w23time开机自动启动，并查看其状态：\nSet-Service w32time -StartupType Automatic\nGet-Service w32time | Select-Object -Property Name, StartType, Status\n接下来查看w32time的配置：\nw32tm /query /configuration\n应该会在输出的[TimeProviders]部分看到类似下面的内容：\nNtpServer (本地)\nDllName: C:\\WINDOWS\\system32\\w32time.dll (本地)\nEnabled: 1 (本地)\n向 NTP 服务器同步时间\n首先启动w32time服务，并让它开机自动启动：\nStart-Service w32time\nSet-Service w32time -StartupType Automatic\nGet-Service w32time | Select-Object -Property Name, StartType, Status\n然后运行下面的命令配置 NTP 客户端要向哪个服务器同步时间：\n$ntpServer = \"127.0.0.1\"\nw32tm /config /manualpeerlist:$ntpServer /syncfromflags:manual /reliable:yes /update\n最后运行下面的命令来强制 NTP 客户端立即同步时间：\nSet-Date (Get-Date).AddSeconds(-5) # 将本地时间提前5秒\nw32tm /resync /force\n这里要将本地时间提前 5 秒是为了确保w32time立即同步时间。经过测试，w32time的逻辑是如果时间相差不多就不会立即同步，所以我们需要故意将本地时间弄得不准，这样才能强制同步。\n可以通过下面的命令来检验同步结果：\nw32tm /stripchart /computer:$ntpServer /samples:5\n所看到的输出应该是类似下面这样的：\n09:16:23, d:+00.0004502s o:+00.0001461s  [                           *                           ]\n其中o:代表 offset，也就是本地时间和 NTP 服务器时间的差值（当然这是个估计值）。如果这个值较低，那么就说明同步成功了。\n关于 Windows NTP 实现的精度\n根据 Windows 官方文档，在 Windows 8 之前，Windows 的 NTP 实现并不标准，因此精度很差，只能达到秒的量级。而在 Windows 10 之后，微软改进了 NTP 服务的实现，现在的精度可以达到毫秒级别。具体可以参考以下文档：\n\nConfiguring systems for high accuracy\nSupport boundary for high-accuracy time（这个文档详细描述了达到不同精度的环境要求。）\n\n","tags":[]},{"title":"Windows的程序文件路径","date":"2025-01-01","url":"/posts/windows-program-files","content":"在 Windows 系统中，全局的（对于所有用户的）程序文件一般存放在C:\\Program Files目录（64 位程序）或者C:\\Program Files (x86)目录（32 位程序）下。这两个目录也有对应的环境变量，分别是%PROGRAMFILES%和%PROGRAMFILES(X86)%。\n这两个目录下的文件受到系统保护，除非有管理员权限，否则只能读取，不能修改。这就是为什么软件在全局安装的时候一定会请求管理员权限。\n程序运行过程中，有时候会需要创建或修改一些文件，比如配置文件或者临时文件等等，这种情况下，如果将这些文件放在%PROGRAMFILES%下就会很不方便。用户肯定不希望每次运行程序都要授予管理员权限。所以这些文件一般会放在用户目录下，比如C:\\Users\\<username>\\AppData\\Local目录。这个目录也有对应的环境变量，是%LOCALAPPDATA%。\n（除了%LOCALAPPDATA%，还有一个%APPDATA%环境变量，这个变量指向的是C:\\Users\\<username>\\AppData\\Roaming目录，根据微软文档，%APPDATA%中的文件会同步到相同账户的远程主机上。我感觉这个功能应该 99% 的人用不到，所以我一般会用%LOCALAPPDATA%，感觉安全一点。）\n当然，%LOCALAPPDATA%目录也可以用来安装软件，只不过其中的软件只对当前用户可见，这样一来安装、卸载的时候就不需要管理员权限了。\n","tags":[]},{"title":"Windows环境下Python UTF-8编码的一些坑","date":"2024-11-30","url":"/posts/windows-python-utf8","content":"众所周知，Python 3 相比 Python 2 最大的变化之一就是字符串强制使用 UTF-8 编码，但这并不意味着使用 Python 3 就不会有字符编码的问题。在 Python 的世界里，确实一切皆 UTF-8，但是一旦和操作系统交互，就可能会出现问题。\nopen()的默认编码\n在 Unix 系统上，open()函数的默认编码是 UTF-8，但是在 Windows 系统上，open()函数的默认编码是 ANSI（也就是根据 Windows 的地区设置而定，在中文 Windows 上一般是 GBK 编码）。因此，如果在 Windows 系统上要让 open()函数使用 UTF-8 编码，就需要显式指定编码：\nopen('file.txt', encoding='utf-8')\nPython 的 UTF-8 模式\n在PEP 540中，Python 引入了一个新的“UTF-8 模式”。这个模式可以通过以下两种方式启动：\n\n在环境变量中设置PYTHONUTF8=1。\n在启动 Python 解释器时，使用-X utf8参数，例如：\n\npython -X utf8 script.py\n启动 UTF-8 模式后，哪怕是在 Windows 系统上，open()函数也会默认使用 UTF-8 编码。\nsys.stdout 的编码\n本节将讨论 Windows 命令行输出的编码问题，下文中的命令行指的都是 PowerShell。\n在 Windows 环境下，如果启动 Python 解释器的时候没有将 stdout 重定向，那么 sys.stdout 的编码是 UTF-8，否则就会是 ANSI。下面的例子就证明了这一点：\n# test.py\nimport sys\nprint(sys.stdout.encoding)\nPS> python test.py\nutf-8\nPS> python test.py > out.txt\nPS> gc out.txt\ngbk\n要让 sys.stdout 的编码始终是 UTF-8，要么启用上文所提到的 UTF-8 模式，要么就要在代码中显式指定编码：\nimport sys\nsys.stdout.reconfigure(encoding='utf-8')\n不过，真正麻烦的地方才刚刚开始。在重定向 stdout 之后，不仅要让 Python 输出 UTF-8，还要让命令行知道输出的是 UTF-8，否则依然会出现乱码。这可以通过指定Out-File命令的-Encoding参数来实现，但这里又有一个奇怪的地方，就是要指定-Encoding为default（也就是 ANSI）才不会乱码。具体的原因我也不太清楚。\npython test.py | Out-File -Encoding default out.txt\n如果觉得每次都要这样写很麻烦，也可以通过指定$PSDefaultParameterValues来让Out-File的-Encoding参数默认为default：\n$PSDefaultParameterValues['Out-File:Encoding'] = 'default'\npython test.py | Out-File out.txt\n可以通过编辑$PROFILE文件来让这个设置永久生效：\nif (-not (Test-Path $PROFILE)) {\n    New-Item $PROFILE -ItemType File -Force\n}\nAdd-Content -Path $PROFILE -Value \"`$PSDefaultParameterValues['Out-File:Encoding'] = 'default'\"\n这里还有一个坑，就是 PowerShell 7 开始，这个default参数被改名成了ansi，所以要注意版本。不过目前 Windows 11 自带的 PowerShell 还是 5.x 版本。\n这里提供一个我写的 PowerShell 函数My-Out-File，可以自动根据版本来设置-Encoding：\nfunction My-Out-File {\n    param (\n        [string]$Path,\n\n        [Parameter(ValueFromPipeline = $true)]\n        [string]$InputString\n    )\n\n    $version = $PSVersionTable.PSVersion.Major\n\n    if ($version -eq 5) {\n        Write-Output $InputString | Out-File -FilePath $Path -Encoding default\n    } elseif ($version -ge 7) {\n        Write-Output $InputString | Out-File -FilePath $Path -Encoding ansi\n    } else {\n        Write-Host \"Unsupported PowerShell version: $version\"\n    }\n}\n","tags":[]},{"title":"主机与WSL虚拟机的网络通信","date":"2025-03-16","url":"/posts/wsl-network","content":"主机到WSL\n在WSL中，执行以下命令可以获得WSL的IP地址：\nhostname -I\n主机可以通过这个IP地址连接到WSL。\nWSL到主机\n在WSL中，执行以下命令可以获得主机的IP地址：\nip route\n主机的IP地址在default via后面，也就是默认路由的下一跳地址。\n也可以通过以下命令直接将结果提取出来：\nip route | grep default | awk '{print $3}'\nWSL中可以通过这个IP地址连接到主机。\n","tags":["WSL"]}]